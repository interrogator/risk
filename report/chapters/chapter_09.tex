%!TEX root = ../risk_report.tex

\chapter{Limitations of the study}

	Our methodology was innovative, and involved fitting theories, practices and tools together in novel ways. Through the course of our investigation we noted two major clusters of limitations. The first were issues relating to the performance and epistemological consequences of digital tools used during the investigation. In short, available digital tools may not perform as desired. In the case of parsers, this is generally an incorrect parse. 

	%	%\noindent Also an issue is that Stanford CoreNLP uses phrase structure and dependency grammars, rather than SFG (for which fewer computational resources are presently available). We were thus left with the task of translating systemic ideas into phrase structure grammar and dependency grammar. This process was often time-consuming and counter-intuitive, as well as theoretically difficult to reconcile. In the case of topic modelling, rather than erroneous, results may simply be unhelpful. The modeller is blind to grammatical structure as well as the division of language into three metafunctions. Accordingly, MALLET groups texts based on words that are unlikely to contribute not contribute to the semantic field of the text (\emph{like, please, to, having, could}). Furthermore, MALLET's conceptualisation of topic may differ markedly from a human reader's: in one iteration of the topic modelling, MALLET grouped together articles about African American religious issues and chess, apparently based on the co-occurrence of words like \emph{black}, \emph{white}, \emph{saving} and \emph{bishop}.

	The second major issue unearthed during the investigation concerned the size of the dataset, which, aside from being simply computationally intensive, was also so large that it constrained the kinds of analytical methods available to us. With 29 annual subcorpora, as well as three topic subcorpora, we struggled to simultaneously maintain a focus on minute changes in lexicogrammar and to connect change generally to events of interest to sociologists. Indeed, though instantiations of risk words may react to current events, further subdivision of the corpus into weekly/monthly subcorpora proved too unwieldy. A similar investigation could be carried out on one subcorpus alone, divided into weeks or months, in order to better assess the influence of individual events. The richness of the data also prevented direct comparison of more risk fields, with only a cursory treatment of government and health risks given here. A final issue caused by issues of data size was that we were unable to manually check each

	 Search query output was manually read to determine that the correct features were being located. What was missing as a result of parsing problems or query design likely went unnoticed amongst the streams of text. By the conclusion of the interrogation, millions of clauses had been manipulated, millions of features extracted and counted---mistakes are unfortunately bound to remain.

\section{The limits of lexicogrammatical querying}

        A major issue we face, and have not dealt with head-on, is the potential for similar discourse-semantic meanings to be made via a number of different kinds of lexicogrammatical arrangements:

        \begin{enumerate} [before=\itshape,font=\normalfont] \setlength\itemsep{0em} \small
            \item Risked money was lost
            \item They risked their money
            \item They risked their savings
            \item Money was risked
            \item The money, which they risked, was lost.
            \item They had money. They risked it.
        \end{enumerate}

        Each of these hypothetical examples communicates that money was risked, but through different grammatical strategies, ranging from the group level (Ex. 1) to the clause-complex (Ex. 6). With great difficulty, we could construct a query that matches every one of these results, or merge the results of a number of searches. As the queries grow in complexity, however, undesirably results may creep in: a query matching \emph{money} in the above cases would also likely match \emph{death} in \emph{They risked death}, despite the fact that one is the risked object and one is the potential harm. Determining the proper functional role is simple for human coders, but the number of results in need of categorisation is often far from trivial. 

        %Furthermore, in Examples 5 and 6 (in which meanings are made across clauses) are at the very limits of even the most sophisticated kinds of grammatical and semantic annotation tools available today.

        The second major issue is the converse: counted in many our automatic queries are many examples in which the money was never risked:

        \begin{enumerate} [before=\itshape,font=\normalfont] \setlength\itemsep{0em} \small
            \item They would have risked their money
            \item They didn't risk their money
            \item Risking money was a terrible idea, so they didn't do it.
            \item Don't risk their money.
        \end{enumerate}

        Few corpus-based studies of discourse have attempted to distinguish between these kinds of meanings automatically. Though many grammars account for the notions of \emph{possibility}, \emph{counterfactuality} or \emph{negation} presented above, how to use these meanings to include\slash exclude matches has yet to be determined.

        It must also be noted, of course, that any study of text corpora necessarily involves removing text from the actuality in which it was produced. Though we can be attuned to the nature of written news journalism, we have not been able to account for meanings made multimodally (through adjacent images, advertisements, etc.). Though perhaps not a critical issue in studies of news corpora, it is nontheless important to acknowledge that in some sense we have been studying \emph{text}, rather than \emph{texts}. Synthesis of corpus findings with in-depth analyses of individual articles, or of the influence of the media production process, would no doubt improve our ability to generalise our results. Indeed, future research incorporating these perspectives is planned.

        % Predictive applications of Big Data/corpus linguistic methods have already been discussed: \citeA{michel_quantitative_2011} and \citeA{leetaru_culturomics_2011} argue that nuanced mining of large quantities of language can potentially predict civil uprisings such as those seen in the Arab Spring, for example. It must be remembered that these studies have been criticised for their far-reaching conclusions \cite<e.g.>{zimmer_when_????}, and of course that predictive applications of corpus linguistics to date have had the benefit of hindsight. Interpreting peaks and troughs in particular kinds of language is also far from straightforward: increasing numbers of risk words before 9\slash 11 could be interpreted as either a possible predictor of the event or as evidence that the event did not itself cause an increase in the use of risk words. As such, we remain cautiously optimistic about predictive applications. More practically, it seems that such applications are feasible only when there is little delay between text production and text analysis: automated analysis of language circulated via the Web seems a much more sensible starting point for predictive work than static corpora of digitised newspapers and books.


\section{Conclusions}

    Instantiation of risk words is linked to real-world events: the beginnings of the AIDS epidemic are accompanied by a spike in health risk discourse; 9\slash 11 appears to be a catalyst for increasing discussion of risks and threats of terror and war. It remains very difficult, however, to fully disentangle the constructive-responsive relationship between real-world events and instantiation of particular concepts in language. Broader ideologies and social movements may indeed be more reliable predictors of linguistic change.

    \begin{itemize}
    \item We found that it is very difficult to pinpoint the effect of individual events on risk word usage in the NYT as a whole.
        \item Our interrogation of economics, health and politics articles turned up clearer evidence of the effect of real events.
        \item Even so, our approach to the creation of the subcorpora was simple, and the topics were very broad. Manually selection of automatically located articles for more specific topics would likely result in clearer indications of the effects of events on risk word usage.
        \item The main thrust of our approach was to investigate the sum total of NYT articles during the sampling years.
        \item While political risks peaked during US election times, this could not be observed when analysing the corpus as a whole.
        \item Accordingly, our findings pointed more toward the influence of broader social movements than to specific events.
        \item We interpreted many of the changes in the behaviour of risk words as evidence for \textbf{neoliberalism} and \textbf{reflexive modernity}.
        \item Indeed, the corpus developed for this study could be reused in a multitude of ways to 
        \item Finally, it must be borne in mind that the NYT is merely one newspaper, and newspapers are merely one genre
        \item We selected NYT for its size, consistency, the availability of digitised content, and its influence in global discourse.
        \item Most obviously, the emergence of the Web challenges this set of criteria in a number of ways. Popular social networks, as well as the public web, produce exponentially more content than a single newspaper.
        \item Mining global news or blog posts via RSS feeds, or Tweets via the Twitter API, allows the quantitative analysis of voices typically marginalised or absent within mainstream media.
    \end{itemize}
    
%\bibliography{../../references/libwin}