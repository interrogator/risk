%!TEX root = ../risk_report.tex

\chapter{Limitations and future directions}


Broadly, our project synthesised \emph{corpus assisted discourse studies} as a \textbf{methodology}, \emph{systemic functional linguistics} as a \textbf{theory of language}, and \emph{sociological accounts of risk} as \textbf{a set of related assumptions about risk}. Our overarching aim is to combine the theories and methods of these areas to provide an empirical account of the ways in the discourse-semantics of risk have undergone longitudinal change.

% ?

Methodologically, our study has demonstrated the potential for large annotated corpora from a single source to provide empirical evidence for key claims in sociological literature. Though ours is not the first study of discourse using linguistic corpora, it is novel in two key respects. First, by creating a corpus comprised of annual subcorpora, and by interrogating each subcorpus in turn, we can learn not only about how language is used to communicate risk, but also how risk language has changed longitudinally. Second, our study takes advantage of major developments within computational linguistics\slash natural language processing. Most obvious is the use of constituency and dependency parsing, but also in this category is the development of an IPython interface for manipulating the corpus and visualising the output of queries. These characteristics both allow far more nuanced kinds of investigation than the practices generally seen in CADS (keywording, collocation, concordancing, etc.), and assist in creating reproducible, transparent, open-source humanities\slash social science research.

\section{Limitations of scope}

In order to fulfill these aims, we necessarily limited our investigation's scope. The first main issue of scope was our choice of print news data from a single publication. The advantages of this kind of text are many: the NYT is a widely read, well-known, and influential publication. The homogeneity of its language and its consistent structure in many ways facilitate longitudinal and quantitative analysis. The drawback of this kind of data, however, is that we can say little about how risk language works within other kinds of communication. Accordingly, we have made no effort to measure the importance or weight of print journalism against other text types in which risk language occurs, such as film and television, various online media or casual spoken\slash written conversation.

There were also import constraints imposed both by our chosen theory of language. Though SFL has proven useful as a framework for analysing how language is drawn upon as a resource for making particular kinds of meanings, it is also a theory which has little to say about language and cognition. This was suitable for our particular investigation, as we cannot possibly determine either the authors' intent behind, nor the readers' interpretations of, the thousands of articles being analysed. Accordingly, we did not attempt to draw links between risk language in the NYT and the ways in which risk is cognitively understood by writers or readers. We suggest that the various strengths of different functional accounts of language can work in tandem, however, and thus welcome future insights from cognitive approaches to risk.

A third and final constraint is our selection of linguistic phenomena for detailed analysis. Primarily, we focus on the experiential and interpersonal dimensions of risk language. The third key component of language is its textual dimension: how language is reflexively organised into meaningful, coherent units. Though our decisions here were guided by the fact that risk as a word does not tend to play important roles in building cohesion and coherence in narratives, we readily admit that more detailed analysis of the role of risk within this dimension may yield important insights that we have not uncovered. Tracking whether risk words shift longitudinally within the textual dimension (between given and new information within a clause, for example) may also be able to show us the extent to which people are aquainted with the notion of risk itself.

\section{Shortcomings in natural language processing tools}

    A major issue in our study relates to the performance and epistemological consequences of digital tools used during the investigation. In short, available digital tools may not perform as desired. Parsers remain far from perfect, and innumerable mistakes in parsing are present in our dataset. What was missing in our results as a result of parsing problems or query design likely went unnoticed amongst the streams of text. By the conclusion of the interrogation, millions of clauses had been manipulated, and millions of features extracted and counted. Accordinglyt, oversights and mistakes are unfortunately bound to remain.

    A related issue is that the parser used here---Stanford CoreNLP---relies on phrase structure and dependency grammars, rather than systemic functional grammar (for which fewer computational resources are presently available). We were thus left with the task of translating systemic-functional concepts into phrase structure grammar and dependency grammar. This process was often time-consuming and counter-intuitive, as well as theoretically difficult to reconcile.

    % In the case of topic modelling, rather than erroneous, results may simply be unhelpful. The modeller is blind to grammatical structure as well as the division of language into three metafunctions. Accordingly, MALLET groups texts based on words that are unlikely to contribute not contribute to the semantic field of the text (\emph{like, please, to, having, could}). Furthermore, MALLET's conceptualisation of topic may differ markedly from a human reader's: in one iteration of the topic modelling, MALLET grouped together articles about African American religious issues and chess, apparently based on the co-occurrence of words like \emph{black}, \emph{white}, \emph{saving} and \emph{bishop}.

    The second major issue unearthed during the investigation concerned the size of the dataset, which, aside from being simply computationally intensive, was also so large that it constrained the kinds of analytical methods available to us. With 29 annual subcorpora, as well as three topic subcorpora, we struggled to simultaneously maintain a focus on minute changes in lexicogrammar and to connect change generally to events of interest to sociologists. Indeed, though instantiations of risk words may react to current events, further subdivision of the corpus into weekly/monthly subcorpora proved too unwieldy. A similar investigation could be carried out on one subcorpus alone, divided into weeks or months, in order to better assess the influence of individual events. The richness of the data also prevented direct comparison of more risk fields, with only a basic treatment of health risks given here. 

    \section{The limits of lexicogrammatical querying}

        A major issue we faced during our investigation, and did not deal with directly, is the potential for similar discourse-semantic meanings to be made via a number of different kinds of lexicogrammatical arrangements. Consider the following invented examples:

        \begin{enumerate} [before=\itshape,font=\normalfont] \setlength\itemsep{0em} \small
            \item They risked their money
            \item Risked money was lost
            \item They risked their savings
            \item The risk of money loss was there
            \item She took her money from her purse and risked it.
            \item The money, which they risked, was lost.
            \item They had money. They risked it.
        \end{enumerate}
        %
        Each of these examples communicates the same kind of semantic meaning---that money was risked---but through different grammatical strategies, ranging from the group level (Ex. 1) to the clause-complex (Ex. 7). Our analyses typically dealt with the most common, or \emph{congruent}, kinds of realisations, but at the expense of meanings made incongruently, or above the level of the clause. With great difficulty, we could construct a query that matches every one of these results, or merge the results of a number of searches. As the queries grow in complexity, however, undesirable results may creep in: a query matching \emph{money} in the above cases would also likely match \emph{death} in \emph{They risked death}, despite the fact that one is the risked object and one is the potential harm. Determining the proper functional role in the cases above is very simple for human coders, but the number of results in need of categorisation is often far from trivial. Limited by both the ability of current parsers and by constraints of scope, we found ourselves largely unable to devise methods for accounting for incongruence in risk language during automated querying. As a result, our analysis was restricted for the most part to meanings that were being made in the most probable, normative ways.

        The second major issue is the exact converse scenatio: counted together in many our automatic queries are many examples with contradictory semantic meanings. Continuing our example of money loss, consider the following:

        \begin{enumerate} [before=\itshape,font=\normalfont] \setlength\itemsep{0em} \small
            \item They would have risked their money
            \item They didn't risk their money
            \item Risking money was a terrible idea, so they didn't do it.
            \item Don't risk their money.
        \end{enumerate}
        %
        In each of these cases, money was not necessarily lost. Lexicogrammatical querying, however, would simply count \emph{money} as the \emph{risked\slash lost thing}. Though we were careful not to conflate our abstracted results with occurrences of particular events (money loss), we did not attempt to determine whether certain things were more often either hypothetically or really risked.

        Our approach is not unique in this sense: few corpus-based studies of discourse have attempted to distinguish between these kinds of meanings automatically. Though many grammars account for the notions of \emph{possibility}, \emph{counterfactuality} or \emph{negation} presented above, how to use these meanings to include\slash exclude matches has for the most part yet to be determined.

        It must also be noted, of course, that any study of text corpora necessarily involves removing text from the actuality in which it was produced. Though we can be attuned to the nature of written news journalism, we have not been able to account for meanings made multimodally (through adjacent images, advertisements, etc.). Though perhaps not a critical issue in studies of news corpora, it is nontheless important to acknowledge that in some sense we have been studying \emph{text}, rather than \emph{texts}. Synthesis of corpus findings with in-depth analyses of individual articles, or of the influence of the media production process, would no doubt improve our ability to generalise our results. Indeed, future research incorporating these perspectives is planned.

        % Predictive applications of Big Data/corpus linguistic methods have already been discussed: \citeA{michel_quantitative_2011} and \citeA{leetaru_culturomics_2011} argue that nuanced mining of large quantities of language can potentially predict civil uprisings such as those seen in the Arab Spring, for example. It must be remembered that these studies have been criticised for their far-reaching conclusions \cite<e.g.>{zimmer_when_????}, and of course that predictive applications of corpus linguistics to date have had the benefit of hindsight. Interpreting peaks and troughs in particular kinds of language is also far from straightforward: increasing numbers of risk words before 9\slash 11 could be interpreted as either a possible predictor of the event or as evidence that the event did not itself cause an increase in the use of risk words. As such, we remain cautiously optimistic about predictive applications. More practically, it seems that such applications are feasible only when there is little delay between text production and text analysis: automated analysis of language circulated via the Web seems a much more sensible starting point for predictive work than static corpora of digitised newspapers and books.

\section{Research agendas}


The tools and methods here could easily be operationalised with new datasets, allowing for research into changes in risk semantics at differnet points in time, or into regional\slash multilingual differences.

Of course, further studies need not be

Though we limited our analysis to risk words and their co-text, resources such as The New York Times Annotated Corpus present the possibility of analysis of any other lexical items and\slash or grammatical structures. Sociological hypotheses concerning the relationship between risk and close synonyms, such as danger or threat, or between risk and key events, such as terrorism, could be studied with a great deal of specificity with a larger corpus. Such a resource could also serve as a more authoritative reference corpus.

Our ability to understand changes in risk semantics in the three topic subcorpora was limited by the size of these datasets. Expanding these datasets to include all articles, rather than just those contaning a risk word, would allow more 

%It should also be noted that the kinds of tasks needed for analysis of proper nouns in particular (selecting particular tokens or thematic clusters of tokens, merging results, zooming in to particular spans of time, etc.) are key components of the corpus analysis toolkit and IPython Notebook interface (\url{https://github.com/interrogator/risk}). Exploration using the purpose-built interface reveals that many topics are present in the data, but are hidden by the frequency of the very prominent examples charted here. Searching for \emph{Yugoslavia}, \emph{Bosnia}, \emph{Kosovo}, \emph{Serbia}, \emph{Mil{\u o}sevi{\'c}}, \emph{\textsc{NATO}}, (etc.), reveal expected peaks at different stages of the 1990s, despite falling outside of the range of the visualisations presented here. Given that the dataset and toolkit are open-sourced, we encourage researchers to conduct further, more specific analysis of risk language and either particular people, places and events, or risk language and topics such as health, politics, economics or sport.

We can identify three major methodological 
that would increase the ability to form reliable conclusions about risk language in news journalism

\begin{enumerate}
    \item Create a larger corpus (from more sources, points in time, etc.)
    \item Utilise entire articles, rather than only those matching criteria
    \item Explore scientific technological innovations (e.g. in Python modules for Time Series Analysis, new kinds of parsers, etc.)
\end{enumerate}

Theoretically, there are far more avenues

Key sociological claims need to be addressed in detail.

The increasing synonymy of risk and threat cannot be accurately measured in a corpus constructed of risk words and their co-text.

\section{Conclusions}

    %Instantiation of risk words is linked to real-world events: the beginnings of the AIDS epidemic are accompanied by a spike in health risk discourse; 9\slash 11 appears to be a catalyst for increasing discussion of risks and threats of terror and war. It remains very difficult, however, to fully disentangle the constructive-responsive relationship between real-world events and instantiation of particular concepts in language. Broader ideologies and social movements may indeed be more reliable predictors of linguistic change.

    %\begin{itemize}
    %\item We found that it is very difficult to pinpoint the effect of individual events on risk word usage in the NYT as a whole.
        %\item Our interrogation of economics, health and politics articles turned up clearer evidence of the effect of real events.
        %\item Even so, our approach to the creation of the subcorpora was simple, and the topics were very broad. Manually selection of automatically located articles for more specific topics would likely result in clearer indications of the effects of events on risk word usage.
        %\item The main thrust of our approach was to investigate the sum total of NYT articles during the sampling years.
        %\item While political risks peaked during US election times, this could not be observed when analysing the corpus as a whole.
        %\item Accordingly, our findings pointed more toward the influence of broader social movements than to specific events.
        %\item We interpreted many of the changes in the behaviour of risk words as evidence for \textbf{neoliberalism} and \textbf{reflexive modernity}.
        %\item Indeed, the corpus developed for this study could be reused in a multitude of ways to 
        %\item Finally, it must be borne in mind that the NYT is merely one newspaper, and newspapers are merely one genre
        %\item We selected NYT for its size, consistency, the availability of digitised content, and its influence in global discourse.
        %\item Most obviously, the emergence of the Web challenges this set of criteria in a number of ways. Popular social networks, as well as the public web, produce exponentially more content than a single newspaper.
        %\item Mining global news or blog posts via RSS feeds, or Tweets via the Twitter API, allows the quantitative analysis of voices typically marginalised or absent within mainstream media.
    %\end{itemize}
    