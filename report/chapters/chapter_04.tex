%!TEX root = ../risk_report.tex

\chapter{Case study: The New York Times, 1963--2014}

	Our investigation centred on digitised texts from \emph{New York Times} editions in 1963 and between 1987--2014. These texts (defined here as individual, complete chunks of content) are predominantly news articles, but depending on archiving practices, also included in our corpus is text-based advertising, box scores, lists, classifieds, letters to the editor, and so on. More specifically, we were interested in any  containing at least one `risk word'---any lexical item whose root is risk (\emph{risking}, \emph{risky}, \emph{riskers}, etc.) or any adjective or adverb containing this root (e.g. \emph{at-risk}, \emph{risk-laden}, \emph{no-risk}).\endnote{This was operationalised through the case-insensitive regular expression \textbf{\textbackslash brisk.*?\textbackslash b}, where \textbf{\textbackslash b} acts as a word-boundary marker.}~

	We relied on two sources for our data. The \emph{New York Times Annotated Corpus} \cite{sandhaus_new_2008} was used as the source for all articles published between 1987--2006. ProQuest was used to search for and download articles containing a risk word from 2007--2014, alongside some metadata, in HTML format. We also created a subcorpus of articles from NYT 1963 editions through optimal character recognition (OCR) of PDF documents archived by ProQuest as containing a risk word in either metadata (i.e. title, lede) or content. Due to the time-intensive nature of manual correction of OCR, a random sample of one-third (1218 texts) was selected, with paragraphs of texts containing a risk word being manually corrected by hand.

	Article text and any available metadata were extracted from this unstructured source content using \emph{Python}'s \emph{Beautiful Soup} library and added to uniquely named text files in annual subfolders. The kinds of metadata available varied according to the data source: The \emph{New York Times Annotated Corpus} provides a number of potentially valuable metadata fields, such as author, newspaper section, and subject (manually added by trained archivists). We then value-added to this partially annotated corpus in three main ways. First, keywords and clusters for each article were calculated using \emph{Spindle} \cite<see>{puerto_spindle_2012} and added as metadata fields. Second, \emph{MALLET} \cite<see>{mccallum_mallet:_2002}, a topic modelling tool, used LDA to algorithmically assign `topics' to each article. The topics and their strengths were added as a metadata field. Finally, we used the \emph{Stanford CoreNLP suite} \cite<see>{manning_stanford_2014} to parse each risk token and its co-text for grammatical structure and dependencies.\endnote{As a part of an ongoing Australian Digital Humanities initiative, since the beginning of our analysis, we have been allocated resources for creating and cloud-hosting a much larger corpus. All 1.8 million articles from the \emph{New York Times Annotated Corpus} are currently being turned into an identically structured, though dramatically larger, cloud-hosted corpus. In planned future research, interrogation of this corpus will be used to determine whether trends in the behaviour of risk words were localised to the word itself, or to general stylistic/language change in the \emph{New York Times}. More details on this project are to be presented in Zinn and McDonald, forthcoming.}~

	A key strength of the methodology is that subcorpora based on article or metadata attributes can can be easily created and compared. Our interest was in creating a small set of topic-specific corpora in order to look for changes in risk word behaviour within specific fields of discourse. As a case study, we decided to focus on three broadly defined topics: \emph{economy}, \emph{health} and \emph{politics}. Librarian-added metadata concerning article topic/category (MC metadata field) was used to locate all articles tagged case-insensitive regular expressions \texttt{\textbackslash beconom.*}, \texttt{\textbackslash bhealth.*} or \texttt{\textbackslash bpolitic.*}.\endnote{We tried a number of strategies for collecting topic subcorpora, such as exploiting topic modeller and keyword metadata. Ultimately, however, we relied on the hand-classification. A limitation of the selected approach is that---an article collected in the health subcorpus was tagged with `Livestock health', for example. Similarly, article categories may be lacking: Figure \ref{fig:exfile} is tagged only with MENINGITIS, and thus is not included in our health subcorpus. More obviously, 1963 articles had not been classified this way, and thus do not feature in the three topic subcorpora.} 

	%\endnote{It would also be possible to create corpora of subtopics: from the politics domain, we could quickly find articles in which Clinton, Bush or Obama are keywords and look for differences in the behaviour of risk words for each politician. Though this seems an interesting area for future research, it was beyond our scope.}~


	\begin{table}\footnotesize \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Tag} & \textbf{Content}  \\ \hline
    MA    & Author(s)       \\ \hline
    MC    & Librarian-added category tags     \\ \hline
    MD    & Date of publication        \\ \hline
    MI    & Unique identifier       \\ \hline
    MK   & MALLET topic        \\ \hline
    MM    & Manually annotated topic  \\ \hline
    MP    & Section of newspaper \\ \hline
    MS    & Risk concordance line  \\ \hline
    MT    & Article title        \\ \hline
    MU    & URL for article         \\ \hline
    MZ    & Annotator comment(s)       \\ \hline
    \end{tabular}
    \caption{Metadata tags and content}
\end{table}
	
	We used some of the metadata fields to identify and remove listings (of best-selling books, plays, TV guides, etc.). Reasons for this were threefold. First, the jargon, abbreviations and non-clausal nature of listing language was not handled well by the parser. Second, list content was often repeated verbatim in multiple files, potentially skewing counts. Third, our two data sources archived listings in different ways. Listings were located by querying metadata fields in a number of ways. Files with titles such as \emph{Spare Times}, \emph{Best Sellers}, articles with keywords such as `theater', `listing', or days of the week. If a file contained only a listing, the file was removed. If a risk word appeared only within the list portion of an article, the file was deleted. If a file contained both a body and listing, only the listing was removed.

	After all data processing, we had a 150 million word corpus of nearly 150,000 articles containing a risk word published in the NYT or \url{NYT.com} in 1963, and between 1987 and mid 2014. The corpus had 29 annual subcorpora. The three subcorpora of economics, health and politics articles contained a subset of these articles. A breakdown of the size and composition of each annual subcorpus is provided in Table \ref{tab:stats}. Where necessary, frequency counts in the 1963 subcorpus were multiplied by four, to account for the smaller sample size. Frequency counts for 2014 were multiplied by 1.37 to fill in the uncaptured period between August 18--December 31.

	\begin{table}
	    \centering
	    \footnotesize
    \begin{tabular}{p{1.5cm}|l|l|l|l|}

%\multicolumn{1}{|l}{Annual subcorpora} \\ \hline
%multicolumn{3}{c|}{RESIDUE}
\hline

\multicolumn{1}{|p{1.5cm}|}{Annual subcorpora} & \textbf{Subcorpus} & \textbf{Words} & \textbf{Articles} & \textbf{Risk words}   \\ \hline
~ & \textbf{1963} & 83,188* & 1218 &  1,584  \\ \cline{2-5}
~ & \textbf{1987} & 4,885,883 & 4,878 &  7,690  \\ \cline{2-5}
~ & \textbf{1988} & 4,834,791 & 4,703 &  7,430  \\ \cline{2-5}
~ & \textbf{1989} & 5,059,517 & 4,997 &  7,810  \\ \cline{2-5}
~ & \textbf{1990} & 5,416,187 & 5,250 &  8,244  \\ \cline{2-5}
~ & \textbf{1991} & 4,748,975 & 4,774 &  7,493  \\ \cline{2-5}
~ & \textbf{1992} & 4,923,509 & 4,818 &  7,329  \\ \cline{2-5}
~ & \textbf{1993} & 4,686,181 & 4,615 &  7,330  \\ \cline{2-5}
~ & \textbf{1994} & 4,857,729 & 4,762 &  7,384  \\ \cline{2-5}
~ & \textbf{1995} & 5,130,206 & 5,150 &  7,834  \\ \cline{2-5}
~ & \textbf{1996} & 4,969,911 & 4,773 &  7,257  \\ \cline{2-5}
~ & \textbf{1997} & 5,121,088 & 4,759 &  7,318  \\ \cline{2-5}
~ & \textbf{1998} & 6,085,810 & 5,437 &  8,351  \\ \cline{2-5}
~ & \textbf{1999} & 6,053,731 & 5,392 &  8,248  \\ \cline{2-5}
~ & \textbf{2000} & 6,472,727 & 5,717 &  8,434  \\ \cline{2-5}
~ & \textbf{2001} & 6,603,456 & 5,902 &  8,722  \\ \cline{2-5}
~ & \textbf{2002} & 6,865,631 & 6,423 & 10,288  \\ \cline{2-5}
~ & \textbf{2003} & 6,795,591 & 6,481 & 10,066  \\ \cline{2-5}
~ & \textbf{2004} & 6,776,200 & 6,215 &  9,989  \\ \cline{2-5}
~ & \textbf{2005} & 6,722,240 & 6,191 & 10,031  \\ \cline{2-5}
~ & \textbf{2006} & 6,722,592 & 6,278 &  9,965  \\ \cline{2-5}
~ & \textbf{2007} & 4,757,290** & 5,110 &  8,976  \\ \cline{2-5}
~ & \textbf{2008} & 5,300,254 & 5,384 &  9,645  \\ \cline{2-5}
~ & \textbf{2009} & 4,926,381 & 5,189 &  9,236  \\ \cline{2-5}
~ & \textbf{2010} & 5,443,658 & 5,527 &  9,560  \\ \cline{2-5}
~ & \textbf{2011} & 5,617,002 & 5,773 & 10,055  \\ \cline{2-5}
~ & \textbf{2012} & 5,366,342 & 5,302 &  9,095  \\ \cline{2-5}
~ & \textbf{2013} & 5,271,006 & 5,176 &  9,083  \\ \cline{2-5}
~ & \textbf{2014} & 3,331,580 & 3,310 &  5,635 \\ \cline{2-5}
~ & \textbf{Total} & \textbf{153,828,656} & \textbf{149,504} & \textbf{240,082} \\ \hline
\multicolumn{1}{|p{1.5cm}|}{Topic \mbox{subcorpora}} & \textbf{Subcorpus} & \textbf{Words} & \textbf{Articles} & \textbf{Risk words}   \\ \hline
~ & \textbf{Economics} & 10,489,137 & 8,286 & 32,448 \\ \cline{2-5}
~ & \textbf{Health}    & 8,524,023  & 6,944 & 36,547 \\ \cline{2-5}
~ & \textbf{Politics}  & 9,465,115  & 7,428 & 20,904 \\ \cline{2-5}
~ & \textbf{Total} & \textbf{28,478,275} & \textbf{22,658} & \textbf{89,899} \\ \cline{2-5}
\end{tabular}
    \caption{Subcorpora, their wordcount, file count and number of risk words}
    \label{tab:stats}
    \medskip % induce some separation between caption and explanatory material
\begin{minipage}{0.9\textwidth} % choose width suitably
{\footnotesize *~Only a small window of co-text---usually two sentences either side of the risk word---was preserved in this subcorpus, hence the smaller size of this sample.~\\
**~The drop in word-count here coincides with the switch from NYT Annotated Corpus to ProQuest as the datasource.\par}
\end{minipage}
\end{table}

			%\begin{table}
			%\centering
			%\small
			%\begin{tabular}{|llll|}
			%\hline
			%Economics & 10489137 & 8286 & 32448 \\ \hline
			%Health    & 8524023  & 6944 & 36547 \\ \hline
			%Politics  & 9465115  & 7428 & 20904 \\ \hline
			%\end{tabular}
			%\end{table}

\begin{figure}
\footnotesize
\begin{lstlisting}[breaklines]
<MY>92 0.14 71 0.12</M>
<MV>13 0.26 96 0.21</M>
<MG>11 0.29 3 0.20</M>
<MO>28 0.33 21 0.24</M>
<MS>One family has lost a child and others may be at risk from a deadly brain inflammation, officials warned yesterday</M>
<MJ>center: 45.444118, officials: 28.536198</M>
<MT>New Jersey Daily Briefing;Meningitis Warning Issued</M>
<MC>MENINGITIS</M>
<MU>http://query.nytimes.com/gst/fullpage.html?res=9B06EFDA1239F933A05751C1A963958260</M>
<MF>0819209.xml</M>
<MA>KELLER, SUSAN JO</M>
<MD>1995-12-30</M>
One family has lost a child and others may be at risk from a deadly brain inflammation, officials warned yesterday. Bacterial meningitis recently killed a baby who attended the Center day-care program, officials say. They are urging parents and staff at the Center to contact their doctors or a hospital emergency room.
\end{lstlisting}
\caption{Example file: NYT-1995-12-30-10.txt}
\label{fig:exfile}
\end{figure}

\section{Tools and interface used for corpus interrogation}

Special tools needed to be developed to work with the very large dataset of both raw NYT articles and parsed paragraphs containing a risk word. Given a well-established history of use within humanities and social sciences, as well as a particular strength in working with linguistic data, we developed a Python-based toolkit for querying our data and visualising query results. Our purpose-built toolkit provided the ability to quickly search each subcorpus of our data and generate useful visualisations of results. Though many parts of the toolkit were designed with more general Digital Humanities projects in mind, certain components of the toolkit were designed exclusively to aid in our particular investigation (projection of counts from 1963 and 2014; automatically stripping names and titles from U.S. politician names, etc.). The specific functions and their purpose are outlined in Table \ref{tab:pyfunc}, with a simple example of a function shown in Figure \ref{fig:code}. More detailed explanations and demonstrations are provided at \url{http://nbviewer.ipython.org/github/interrogator/risk/blob/master/risk.ipynb}; the repository of code itself is available via \emph{GitHub} (\url{https://github.com/interrogator/risk}), where it can freely be downloaded, or duplicated and modified.

\begin{table}[h!]
\small
\centering
\begin{tabularx}{0.5\textwidth}{|l|X|} \hline

\textbf{Function name} &  \textbf{Purpose}     \\ \hline
\texttt{interrogator()}  &  interrogate parsed corpora          \\ \hline
\texttt{dependencies()}  &  interrogate parsed corpora for dependency info (presented later)          \\ \hline
\texttt{plotter()}       &  visualise \texttt{interrogator()} results  \\ \hline
\texttt{table()}          &  return \texttt{plotter()} results as table  \\ \hline
\texttt{quickview()}     &  view \texttt{interrogator()} results       \\ \hline
\texttt{tally()}       &  get total frequencies for \texttt{interrogator()} results       \\ \hline
\texttt{surgeon()}       &  edit \texttt{interrogator()} results       \\ \hline
\texttt{merger()}       &  merge \texttt{interrogator()} results       \\ \hline
\texttt{conc()}          &  complex concordancing of subcorpora  \\ \hline
\texttt{keywords()}          &  get keywords and ngrams from \texttt{conc()} output  \\ \hline
\texttt{collocates()}          &  get collocates from \texttt{conc()} output \\ \hline
\texttt{quicktree()}          &  visually represent a parse tree  \\ \hline
\texttt{searchtree()}          &  search a parse tree with a Tregex query  \\ \hline
\end{tabularx}
\caption{Python functions developed for our investigation}
\label{tab:pyfunc}
\end{table}

\begin{figure}
\centering
\begin{minipage}[c]{.75\textwidth}
\centering
\begin{minted}[fontsize=\small,linenos=true]{python}
def collocates(data, nbest = 30, window = 5):
    """Feed this a csv file generated with conc() and get collocations"""
    import nltk
    from nltk import collocations
    from nltk.collocations import BigramCollocationFinder
    import re
    f = open(data)
    raw = f.read()
    bad, good = re.compile(r'Entire sentences \(n=[0-9]+\):').split(raw)
    good = unicode(good.lower(), 'utf-8', errors = 'ignore')
    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')
    sents = sent_tokenizer.tokenize(good)
    tokenized_sents = [nltk.word_tokenize(i) for i in sents]
    allwords = []
    # for each sentence,
    for sent in tokenized_sents:
    # for each word,
        for word in sent:
        # make a list of all words
            allwords.append(word)
    bigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_words(allwords, window_size=window)
    ignored_words = nltk.corpus.stopwords.words('english')
    # anything containing letter or number
    regex = r'[A-Za-z0-9]'
    # the n't token
    nonot = r'n\'t'
    # lots of conditions!
    finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() in \
        ignored_words or not re.match(regex, w) or re.match(nonot, w))
    finder.apply_freq_filter(2)
    results = sorted(finder.nbest(bigram_measures.raw_freq, nbest))
    listversion = []
    for index, thecollocation in enumerate(results):
        aslist = [index, thecollocation[0], thecollocation[1]]
        listversion.append(aslist)
    return listversion
\end{minted}
\caption{Python code to find collocates in concordance results}
\label{fig:code}
\end{minipage}
\end{figure}


Finally, we developed an IPython Notebook based interface for using these functions to investigate the NYT corpus (also available cia our GitHub URL above). This served not only as our main platform for interrogating the dataset, but also as a means of dynamically disseminating results without being limited by considerations of space. In being open-source, and in explicitly showing the exact queries used to generate findings, the Notebook ensures both reproducability and transparency of the entirety of our investigation. At the same time, it provides a framework for sophisticated corpus-assisted discourse analysis using cutting-edge digital research tools. Researchers are encouraged to run the Notebook in conjunction with this report, so that they can generate and manipulate our key findings as they see fit.


% \begin{table}
% \centering
% \tiny
% 
% \begin{tabularx}{1.0\textwidth}{|llllllllllllllllllllllllllllll|}
% \hline
% 
% \textbf{Subcorpus}          & \textbf{1963}  & \textbf{1987}    & \textbf{1988}    & \textbf{1989}    & \textbf{1990}    & \textbf{1991}    & \textbf{1992}    & \textbf{1993}    & \textbf{1994}    & \textbf{1995}    & \textbf{1996}    & \textbf{1997}    & \textbf{1998}    & \textbf{1999}    & \textbf{2000}    & \textbf{2001}    & \textbf{2002}    & \textbf{2003}    & \textbf{2004}    & \textbf{2005}    & \textbf{2006}    & \textbf{2007}    & \textbf{2008}    & \textbf{2009}    & \textbf{2010}    & \textbf{2011}    & \textbf{2012}    & \textbf{2013}    & \textbf{2014}    \\ \hline
% \textbf{Wordcount}     & 83188 & 4885883 & 4834791 & 5059517 & 5416187 & 4748975 & 4923509 & 4686181 & 4857729 & 5130206 & 4969911 & 5121088 & 6085810 & 6053731 & 6472727 & 6603456 & 6865631 & 6795591 & 6776200 & 6722240 & 6722592 & 4757290 & 5300254 & 4926381 & 5443658 & 5617002 & 5366342 & 5271006 & 3331580 \\
% \textbf{Article count} & 1218  & 4878    & 4703    & 4997    & 5250    & 4774    & 4818    & 4615    & 4762    & 5150    & 4773    & 4759    & 5437    & 5392    & 5717    & 5902    & 6423    & 6481    & 6215    & 6191    & 6278    & 5110    & 5384    & 5189    & 5527    & 5773    & 5302    & 5176    & 3310    \\
% \textbf{Risk words}    & 1584  & 7690    & 7430    & 7810    & 8244    & 7493    & 7329    & 7330    & 7384    & 7834    & 7257    & 7318    & 8351    & 8248    & 8434    & 8722    & 10288   & 10066   & 9989    & 10031   & 9965    & 8976    & 9645    & 9236    & 9560    & 10055   & 9095    & 9083    & 5635    \\
% \hline
% \end{tabularx}
%     \caption{Subcorpora their wordcount, file count and risk token count}
% \end{table}
% 
% 	%Figure charting articles, words, risk tokens by year here \label{fig:stats}
		
%\bibliography{../references/libwin}