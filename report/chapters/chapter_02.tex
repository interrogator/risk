%!TEX root = ../risk_report.tex

\chapter{The case study: \emph{The New York Times}, 1963, 1987--2014}

\section{Selecting \emph{The New York Times} as a case study}

There is good evidence that the risk semantic has become more common in societal discourses and practices. A direct count of articles which contain a risk token at least once showed how the dynamic of risk developed in many countries after WW2 (Zinn 2011). It clearly shows how risk is mainly a phenomenon that developed a particular dynamic after WW2 in particular in the late 1980s. It also shows that the risk semantic had been around for quite a while without a clear dynamic. This is interesting and invites more long term investigations.

With the current study we wanted to examine in much more detail whether during a historical relatively short period from 1987 to 2014 (we used a sample of the 1963 volume to contrast with the later years) significant shifts can be observed using much more sophisticated research strategies than used in earlier corpus based approaches on the risk semantic (e.g. Hamilton et al, 2007).

Therefore we selected only one newspaper---The New York Times---as a case study after careful consideration of other available resources. We aimed to find a resource that allows longitudinal analysis of long term social change with a limited number of intervening factors. We were looking for a paper which provided a high quality digitised archive and a central news institution over the centuries.

The (London) Times and the NYT seem suitable because of their important social role within a society. They also fulfil further selection criteria such as wide circulation (not just regional), good accessibility and high data quality. However the NYT has been finally selected because of the central role of the US in the world and the prestige and clout of the NYT. The NYT is a historically central institution of media coverage (Chapman 2005) with a continuously high status and standard of coverage. It is influential, highly circulated and publicly acknowledged news media. It contains extensive coverage of both national and international developments, its digital archive covers all years since WWII and is relatively easy to access.

Available Australian Newspapers such as The Australian or The Age offer similar digitised archives only for recent decades and at higher cost. Long term historical analyses are much more complicated and will be pursued when we have proven our methodology.

The project concentrates on a single newspaper and follows a reproduction logic (Yin 1989) for four reasons:

\begin{enumerate}
\item The `historical change of concepts' (Koselleck 2002) is so general that it can be identified even in specific newspapers though newspaper specific factors have to be considered.
\item A detailed analysis of available newspapers archives by the CI has found that, in the US, only the Washington Post provides a comparable archive. While both show no significant differences in the general increase of the usage of the risk semantic (Zinn 2010, p. 115), access and data management has proven easier and more reliable with the NYT.
\item The case study allows a more detailed analysis of how the change of the newspaper might have influenced the use of risk. A collection of newspapers, as in many linguistic text corpuses would not lead to representative results but would create uncontrolled biases. Instead, the case study of a specific newspaper allows a much more detailed analysis of how change of the newspaper itself, such as a change in leadership or style of news reporting, might have influenced the use of risk.
\item The study limits the amount of data and restricts costs without losing significant outcomes. Originally we wanted to compare the volumes 1963, 1988, 2013 of The New York Times. We soon found out about the availability of a high quality data resource, The New York Times Annotated Corpus  which covers all articles published from 1987--mid-2007 and includes substantial metadata and contains 1,130,621,175 words. We complemented this dataset with articles from the NYT online archive up to 2013\slash 14.
\end{enumerate} 
% NYTAC: (http://catalog.ldc.upenn.edu/LDC2008T19)

In order to further validate our results, future research has been planned that will compare our results with more recent data from other US newspapers. Though in the US many newspapers are digitised the main issue is that some papers are strictly PDF while some of these PDFs have the plain text version also available. We identified major newspapers which are suitable for comparative purposes in future research.

\section{Building the \emph{Risk Corpus}}

Our investigation centred on digitised texts from New York Times editions in 1963 and between 1987--2014. These texts (defined here as individual, complete chunks of content) are predominantly news articles, but depending on archiving practices, also included in our corpus is text-based advertising, box scores, lists, classifieds, letters to the editor, and so on. More specifically, we were interested in any containing at least one `risk word'---any lexical item whose root is risk (risking, risky, riskers, etc.) or any adjective or adverb containing this root (e.g. \emph{at-risk}, \emph{risk-laden}, \emph{no-risk}).

We relied on two sources for our data. \emph{The New York Times Annotated Corpus} was used as the source for all articles published between 1987--2006. ProQuest was used to search for and download articles containing a risk word from 2007--2014, alongside some metadata, in HTML format. We also created a subcorpus of articles from NYT 1963 editions through optimal character recognition (OCR) of PDF documents archived by ProQuest as containing a risk word in either metadata (i.e. title, lede) or content. Due to the time-intensive nature of manual correction of OCR, a random sample of one-third (1218 texts) was selected, with paragraphs of texts containing a risk word being manually corrected by hand.\endnote{Efforts are underway to digitise all risk words in 1963 additions, as well as editions from years between 1963 and 1987.}

%Our investigation centred on digitised texts from \emph{New York Times} editions in 1963 and between 1987--2014. These texts (defined here as individual, complete chunks of content) are predominantly news articles, but depending on archiving practices, also included in our corpus is text-based advertising, box scores, lists, classifieds, letters to the editor, and so on. More specifically, we were interested in any  containing at least one `risk word'---any lexical item whose root is risk (\emph{risking}, \emph{risky}, \emph{riskers}, etc.) or any adjective or adverb containing this root (e.g. \emph{at-risk}, \emph{risk-laden}, \emph{no-risk}).\endnote{This was operationalised through the case-insensitive regular expression \textbf{\textbackslash brisk.*?\textbackslash b}, where \textbf{\textbackslash b} acts as a word-boundary marker.}~

%We relied on two sources for our data. The \emph{New York Times Annotated Corpus} \cite{sandhaus_new_2008} was used as the source for all articles published between 1987--2006. ProQuest was used to search for and download articles containing a risk word from 2007--2014, alongside some metadata, in HTML format. We also created a subcorpus of articles from NYT 1963 editions through optimal character recognition (OCR) of PDF documents archived by ProQuest as containing a risk word in either metadata (i.e. title, lede) or content. Due to the time-intensive nature of manual correction of OCR, a random sample of one-third (1218 texts) was selected, with paragraphs of texts containing a risk word being manually corrected by hand.

Article text and any available metadata were extracted from this unstructured source content using \emph{Python}'s \emph{Beautiful Soup} library and added to uniquely named text files in annual subfolders. The kinds of metadata available varied according to the data source: The \emph{New York Times Annotated Corpus} provides a number of potentially valuable metadata fields, such as author, newspaper section, and subject (manually added by trained archivists). These metadata fields provided both human-readable information for use during qualitative analysis of texts, and machine-readable information that could be used to restructure the corpus in future investigations.

We then value-added to this partially annotated corpus in three main ways. First, keywords and clusters for each article were calculated using \emph{Spindle} \cite<see>{puerto_spindle_2012} and added as metadata fields. Second, \emph{MALLET} \cite<see>{mccallum_mallet:_2002}, a topic modelling tool, used LDA to algorithmically assign `topics' to each article. The topics and their strengths were added as a metadata field. Finally, we used the \emph{Stanford CoreNLP suite} \cite<see>{manning_stanford_2014} to parse each risk token and its co-text for grammatical structure and dependencies.\endnote{As a part of an ongoing Australian Digital Humanities initiative, since the beginning of our analysis, we have been allocated resources for creating and cloud-hosting a much larger corpus. All 1.8 million articles from the \emph{New York Times Annotated Corpus} are currently being turned into an identically structured, though dramatically larger, cloud-hosted corpus. In planned future research, interrogation of this corpus will be used to determine whether trends in the behaviour of risk words were localised to the word itself, or to general stylistic/language change in the \emph{New York Times}. More details on this project are to be presented in Zinn and McDonald, forthcoming.}~

A key strength of the methodology is that subcorpora based on article or metadata attributes can can be easily created and compared. Our interest was in creating a small set of topic-specific corpora in order to look for changes in risk word behaviour within a specific field of discourse. As a case study, we decided to focus on health articles. Librarian-added metadata concerning article topic/category (MC metadata field) was used to locate all articles tagged with the case-insensitive Regular Expression \texttt{\textbackslash~bhealth.*}.\endnote{We tried a number of strategies for collecting topic subcorpora, such as exploiting topic modeller and keyword metadata. Ultimately, however, we relied on the hand-classification. A limitation of the selected approach is that---an article collected in the health subcorpus was tagged with `Livestock health', for example. Similarly, article categories may be lacking: Figure \ref{fig:exfile} is tagged only with MENINGITIS, and thus is not included in our health subcorpus. More obviously, 1963 articles had not been classified this way, and thus do not feature in the three topic subcorpora.} 

    %\endnote{It would also be possible to create corpora of subtopics: from the politics domain, we could quickly find articles in which Clinton, Bush or Obama are keywords and look for differences in the behaviour of risk words for each politician. Though this seems an interesting area for future research, it was beyond our scope.}~

    \begin{table}\footnotesize \centering
    \begin{tabular}{ll}
    \toprule
    \textbf{Tag} & \textbf{Content}  \\ \midrule
    MA    & Author(s)       \\ 
    MC    & Librarian-added category tags     \\ 
    MD    & Date of publication        \\ 
    MI    & Unique identifier       \\ 
    MK    & MALLET topic        \\ 
    MM    & Manually annotated topic  \\ 
    MP    & Section of newspaper \\ 
    MS    & Risk concordance line  \\ 
    MT    & Article title        \\ 
    MU    & URL for article         \\ 
    MZ    & Annotator comment(s)       \\ \bottomrule
    \end{tabular}
    \caption{Metadata tags and content}
\end{table}
    
We used some of the metadata fields to identify and remove listings (of best-selling books, plays, TV guides, etc.). Reasons for this were threefold. First, the jargon, abbreviations and non-clausal nature of listing language was not handled well by the parser. Second, list content was often repeated verbatim in multiple files, potentially skewing counts. Third, our two data sources archived listings in different ways. Listings were located by querying metadata fields in a number of ways. Files with titles such as \emph{Spare Times}, \emph{Best Sellers}, articles with keywords such as `theater', `listing', or days of the week. If a file contained only a listing, the file was removed. If a risk word appeared only within the list portion of an article, the file was deleted. If a file contained both a body and listing, only the listing was removed.

After all data processing, we had a 150 million word corpus of nearly 150,000 unique articles containing a risk word published in the NYT or \url{NYT.com} in 1963, and between 1987 and mid 2014. The corpus had 29 annual subcorpora. The health subcorpus contained a subset of 8,524023 words, 6,944 articles and 36,547 risk words. A breakdown of the size and composition of each annual subcorpus is provided in Table \ref{tab:stats}. During analysis, when conducting absolute frequency analysis, frequency counts in the 1963 subcorpus were multiplied by four, to account for the smaller sample size. Frequency counts for 2014 were multiplied by 1.37 to fill in the uncaptured period between August 18--December 31.

\begin{table}
\centering
\footnotesize
\begin{tabular}{p{1.5cm}rrr}

\toprule
\textbf{Subcorpus} & \textbf{Words} & \textbf{Articles} & \textbf{Risk words}  \\ \midrule
\textbf{1963} & 83,188* & 1218 &  1,584  \\ 
\textbf{1987} & 4,885,883 & 4,878 &  7,690  \\ 
\textbf{1988} & 4,834,791 & 4,703 &  7,430  \\ 
\textbf{1989} & 5,059,517 & 4,997 &  7,810  \\ 
\textbf{1990} & 5,416,187 & 5,250 &  8,244  \\ 
\textbf{1991} & 4,748,975 & 4,774 &  7,493  \\ 
\textbf{1992} & 4,923,509 & 4,818 &  7,329  \\ 
\textbf{1993} & 4,686,181 & 4,615 &  7,330  \\ 
\textbf{1994} & 4,857,729 & 4,762 &  7,384  \\ 
\textbf{1995} & 5,130,206 & 5,150 &  7,834  \\ 
\textbf{1996} & 4,969,911 & 4,773 &  7,257  \\ 
\textbf{1997} & 5,121,088 & 4,759 &  7,318  \\ 
\textbf{1998} & 6,085,810 & 5,437 &  8,351  \\ 
\textbf{1999} & 6,053,731 & 5,392 &  8,248  \\ 
\textbf{2000} & 6,472,727 & 5,717 &  8,434  \\ 
\textbf{2001} & 6,603,456 & 5,902 &  8,722  \\ 
\textbf{2002} & 6,865,631 & 6,423 & 10,288  \\ 
\textbf{2003} & 6,795,591 & 6,481 & 10,066  \\ 
\textbf{2004} & 6,776,200 & 6,215 &  9,989  \\ 
\textbf{2005} & 6,722,240 & 6,191 & 10,031  \\ 
\textbf{2006} & 6,722,592 & 6,278 &  9,965  \\ 
\textbf{2007} & 4,757,290** & 5,110 &  8,976  \\ 
\textbf{2008} & 5,300,254 & 5,384 &  9,645  \\ 
\textbf{2009} & 4,926,381 & 5,189 &  9,236  \\ 
\textbf{2010} & 5,443,658 & 5,527 &  9,560  \\ 
\textbf{2011} & 5,617,002 & 5,773 & 10,055  \\ 
\textbf{2012} & 5,366,342 & 5,302 &  9,095  \\ 
\textbf{2013} & 5,271,006 & 5,176 &  9,083  \\ 
\textbf{2014} & 3,331,580 & 3,310 &  5,635 \\ 
\textbf{Total} & \textbf{153,828,656} & \textbf{149,504} & \textbf{240,082} \\ \bottomrule
% \multicolumn{1}{p{1.5cm}}{Topic \mbox{subcorpora}} & \textbf{Subcorpus} & \textbf{Words} & \textbf{Articles} & \textbf{Risk words}   \\ \hline
% ~ & \textbf{Economics} & 10,489,137 & 8,286 & 32,448 \\ \cline{2-5}
% ~ & \textbf{Health}    & 8,524,023  & 6,944 & 36,547 \\ \cline{2-5}
% ~ & \textbf{Politics}  & 9,465,115  & 7,428 & 20,904 \\ \cline{2-5}
%~ & \textbf{Total} & \textbf{28,478,275} & \textbf{22,658} & \textbf{89,899} \\ \cline{2-5}
\end{tabular}
    \caption{Subcorpora, their wordcount, file count and number of risk words}
    \label{tab:stats}
    \medskip % induce some separation between caption and explanatory material
\begin{minipage}{0.9\textwidth} % choose width suitably
{\footnotesize *~Only a small window of co-text---usually two sentences either side of the risk word---was preserved in this subcorpus, hence the smaller size of this sample.~\\
 \mbox{*}\mbox{*}~The drop in word-count here coincides with the switch from NYT Annotated Corpus to ProQuest as the data-source.\par}
\end{minipage}
\end{table}

            %\begin{table}
            %\centering
            %\small
            %\begin{tabular}{llll}
            %\hline
            %Economics & 10489137 & 8286 & 32448 \\ \hline
            %Health    & 8524023  & 6944 & 36547 \\ \hline
            %Politics  & 9465115  & 7428 & 20904 \\ \hline
            %\end{tabular}
            %\end{table}

\begin{figure}
\footnotesize
\begin{lstlisting}[breaklines]
<MY>92 0.14 71 0.12</M>
<MV>13 0.26 96 0.21</M>
<MG>11 0.29 3 0.20</M>
<MO>28 0.33 21 0.24</M>
<MS>One family has lost a child and others may be at risk from a deadly brain inflammation, officials warned yesterday</M>
<MJ>center: 45.444118, officials: 28.536198</M>
<MT>New Jersey Daily Briefing;Meningitis Warning Issued</M>
<MC>MENINGITIS</M>
<MU>http://query.nytimes.com/gst/fullpage.html?res=9B06EFDA1239F933A05751C1A963958260</M>
<MF>0819209.xml</M>
<MA>KELLER, SUSAN JO</M>
<MD>1995-12-30</M>
One family has lost a child and others may be at risk from a deadly brain inflammation, officials warned yesterday. Bacterial meningitis recently killed a baby who attended the Center day-care program, officials say. They are urging parents and staff at the Center to contact their doctors or a hospital emergency room.
\end{lstlisting}
\caption{Example file: NYT-1995-12-30-10.txt}
\label{fig:exfile}
\end{figure}

\section{Tools and interface used for corpus interrogation}

Special tools needed to be developed to work with the very large dataset of both raw NYT articles and parsed paragraphs containing a risk word. Given a well-established history of use within humanities and social sciences, as well as a particular strength in working with linguistic data, we developed a Python-based toolkit for querying our data and visualising query results. Our purpose-built toolkit provided the ability to quickly search each subcorpus of our data, edit the results from our searches, perform concordancing and thematic categorisation, and generate visualisations of results. Though many parts of the toolkit were designed with more general Digital Humanities projects in mind, certain components of the toolkit were designed exclusively to aid in our particular investigation (projection of counts from 1963 and 2014; automatically stripping names and titles from U.S. politician names, etc.). The most important functions and their purpose are outlined in Table \ref{tab:pyfunc}, with a simple example of a function shown in Figure \ref{fig:code}. More detailed explanations and demonstrations are provided at \url{http://nbviewer.ipython.org/github/interrogator/risk/blob/master/risk.ipynb}; the repository of code itself is available via \emph{GitHub} (\url{https://github.com/interrogator/risk}), where it can freely be downloaded, or duplicated and modified.

\begin{table}[h!]
\small
\centering
\begin{tabularx}{0.75\textwidth}{lX} \toprule

\textbf{Function name} &  \textbf{Purpose}     \\ \midrule
\texttt{interrogator()}  &  interrogate parse trees, find keywords, collocates, etc.    \\ 
\texttt{plotter()}       &  visualise \texttt{interrogator()} results  \\ 
\texttt{quickview()}     &  view \texttt{interrogator()} results       \\ 
\texttt{editor()}       &  edit \texttt{interrogator()} results       \\ 
\texttt{conc()}          &  complex concordancing of subcorpora  \\ 
\texttt{collocates()}          &  get collocates from corpus\slash subcorpus\slash concordance lines \\ 
\texttt{quicktree()}          &  visually represent a parse tree  \\ 
\texttt{searchtree()}          &  search a parse tree with a Tregex query  \\ \bottomrule
\end{tabularx}
\caption{Core Python functions developed for our investigation}
\label{tab:pyfunc}
\end{table}

\begin{figure}
\centering
\begin{minipage}[c]{.75\textwidth}
\centering
\begin{minted}[fontsize=\small,linenos=true]{python}
def ngrams(data,
           reference_corpus = 'bnc.p',
           clear = True, 
           printstatus = True, 
           n = 'all',
           **kwargs):
    """Feed this function some data and get its keywords.

    You can use dictmaker() to build a new reference_corpus 
    to serve as reference corpus, or use bnc.p

    A list of what counts as data is available in the 
    docstring of datareader().
    """
    
    import re
    import time
    from time import localtime, strftime
    import pandas as pd
    try:
        from IPython.display import display, clear_output
    except ImportError:
        pass
    from corpkit.keys import keywords_and_ngrams, turn_input_into_counter
    from corpkit.other import datareader
    from dictionaries.stopwords import stopwords as my_stopwords

    loaded_ref_corpus = turn_input_into_counter(reference_corpus)
    
    time = strftime("%H:%M:%S", localtime())
    if printstatus:
        print "\n%s: Generating ngrams... \n" % time
    good = datareader(data, **kwargs)

    regex_nonword_filter = re.compile("[A-Za-z-\']")
    good = [i for i in good if re.search(regex_nonword_filter, i) 
            and i not in my_stopwords] 

    ngrams = keywords_and_ngrams(good, reference_corpus = reference_corpus, 
                                 calc_all = calc_all, show = 'ngrams', **kwargs)

    out = pd.Series([s for k, s in ngrams], index = [k for k, s in ngrams])
    out.name = 'ngrams'

    # print and return
    if clear:
        clear_output()
    if printstatus:
        time = strftime("%H:%M:%S", localtime())
        print '%s: Done! %d results.\n' % (time, len(list(out.index)))
    if n  == 'all':
        n = len(out)
    return out[:n]
\end{minted}
\caption{Python function for getting n-grams from corpus data}
\label{fig:code}
\end{minipage}
\end{figure}
%
Finally, we developed an IPython Notebook based interface for using these functions to investigate the NYT corpus (also available via our GitHub URL above). This served not only as our main platform for interrogating the dataset, but also as a means of dynamically disseminating results without being limited by considerations of space. In being open-source, and in explicitly showing the exact queries used to generate findings, the Notebook ensures both reproducibility and transparency of the entirety of our investigation. At the same time, it provides a framework for sophisticated corpus-assisted discourse analysis using cutting-edge digital research tools. Researchers are encouraged to run the Notebook in conjunction with this report, so that they can generate and manipulate our key findings as they see fit.

% \begin{table}
% \centering
% \tiny
% 
% \begin{tabularx}{1.0\textwidth}{|llllllllllllllllllllllllllllll|}
% \hline
% 
% \textbf{Subcorpus}          & \textbf{1963}  & \textbf{1987}    & \textbf{1988}    & \textbf{1989}    & \textbf{1990}    & \textbf{1991}    & \textbf{1992}    & \textbf{1993}    & \textbf{1994}    & \textbf{1995}    & \textbf{1996}    & \textbf{1997}    & \textbf{1998}    & \textbf{1999}    & \textbf{2000}    & \textbf{2001}    & \textbf{2002}    & \textbf{2003}    & \textbf{2004}    & \textbf{2005}    & \textbf{2006}    & \textbf{2007}    & \textbf{2008}    & \textbf{2009}    & \textbf{2010}    & \textbf{2011}    & \textbf{2012}    & \textbf{2013}    & \textbf{2014}    \\ \hline
% \textbf{Wordcount}     & 83188 & 4885883 & 4834791 & 5059517 & 5416187 & 4748975 & 4923509 & 4686181 & 4857729 & 5130206 & 4969911 & 5121088 & 6085810 & 6053731 & 6472727 & 6603456 & 6865631 & 6795591 & 6776200 & 6722240 & 6722592 & 4757290 & 5300254 & 4926381 & 5443658 & 5617002 & 5366342 & 5271006 & 3331580 \\
% \textbf{Article count} & 1218  & 4878    & 4703    & 4997    & 5250    & 4774    & 4818    & 4615    & 4762    & 5150    & 4773    & 4759    & 5437    & 5392    & 5717    & 5902    & 6423    & 6481    & 6215    & 6191    & 6278    & 5110    & 5384    & 5189    & 5527    & 5773    & 5302    & 5176    & 3310    \\
% \textbf{Risk words}    & 1584  & 7690    & 7430    & 7810    & 8244    & 7493    & 7329    & 7330    & 7384    & 7834    & 7257    & 7318    & 8351    & 8248    & 8434    & 8722    & 10288   & 10066   & 9989    & 10031   & 9965    & 8976    & 9645    & 9236    & 9560    & 10055   & 9095    & 9083    & 5635    \\
% \hline
% \end{tabularx}
%     \caption{Subcorpora their wordcount, file count and risk token count}
% \end{table}
% 
%     %Figure charting articles, words, risk tokens by year here \label{fig:stats}