#!/usr/bin/python

#   Interrogating parsed corpora and plotting the results: dependencies
#   Author: Daniel McDonald

def dependencies(path, options, query, lemmatise = False, test = False, 
    titlefilter = False, lemmatag = False, dep_type = 'basic-dependencies', only_count = False):
    """Uses BeautifulSoup to make list of frequency counts in corpora.
    
    Interrogator investigates sets of Stanford dependencies for complex frequency information. 

    path: path to corpus as string
    options: 
        'funct': get functional label
        'depnum': get index of item within the clause
        'govrole': get role and governor, colon-separated
        query: regex to match
    Lemmatise: lemmatise results
    test: for development, go through only three subcorpora
    titlefilter: strip 'mr, mrs, dr' etc from proper noun strings
    dep_type: specify type of dependencies to search:
                    'basic-dependencies' * best lemmatisation
                    'collapsed-dependencies'
                    'collapsed-ccprocessed-dependencies'

    Note: subcorpora directory names must be numbers only.
    """
    import os
    from bs4 import BeautifulSoup, SoupStrainer
    import collections
    from collections import Counter
    import time
    from time import localtime, strftime
    import re
    import sys
    import string
    import codecs
    from string import digits
    import operator
    import glob
    from corpling_tools.progressbar import ProgressBar
    import gc
    if lemmatise:
        import nltk
        from nltk.stem.wordnet import WordNetLemmatizer
        lmtzr=WordNetLemmatizer()
        # location of words for manual lemmatisation
        from data.dictionaries.manual_lemmatisation import wordlist, deptags
    try:
        from IPython.display import display, clear_output
        have_ipython = True
    except ImportError:
        have_ipython = False
    # define option regexes
    time = strftime("%H:%M:%S", localtime())
    try:
        from IPython.display import display, clear_output
        have_ipython = True
    except ImportError:
        have_ipython = False
    strip_bad = re.compile('[^.a-zA-Z0-9/-:]', re.UNICODE)

    # welcomer
    if options == 'depnum':
        optiontext = 'Number only.'
    elif options == 'funct':
        optiontext = 'Functional role only.'
    elif options == 'govrole':
        optiontext = 'Role and governor.'
    else:
        raise ValueError("Option must be 'depnum', 'funct' or 'govrole'.")
    time = strftime("%H:%M:%S", localtime())
    print "%s: Begining corpus interrogation: %s\n          Query: %s\
    \n          %s\n          Interrogating corpus ... \n" % (time, path, query, optiontext)
    
    def processwords(list_of_matches):
        # encoding
        #matches = [unicode(match, 'utf-8', errors = 'ignore') for match in list_of_matches if type(match) != unicode]
        #matches = [match for match in list_of_matches if type(match) == unicode]
        # remove bad characters
        #matches = [re.sub(strip_bad, "", match) for match in matches]
        #matches = filter(None, matches)
        #lowercasing
        lowered = []
        for match in list_of_matches:
            try:
                lowered.append(match.lower())
            except:
                lowered.append(match)
        #print lowered[:50]
        #matches = [match.lower() for match in matches if any(c.isalpha() for c in match)]
        #matches.append([match for match in matches if not any(c.isalpha() for c in match)])
        if titlefilter:
            lowered = titlefilterer(lowered)
        return lowered

    def titlefilterer(list_of_matches):
        import re
        import nltk
        from data.dictionaries.titlewords import titlewords
        tokenised_list = [nltk.word_tokenize(i) for i in list_of_matches]
        output = []
        for result in tokenised_list:
            head = result[-1]
            non_head = result.index(head) # ???
            title_stripped = [token for token in result[:non_head] if token not in titlewords]
            title_stripped.append(head)
            str_result = ' '.join(title_stripped)
            output.append(str_result)
        return output


    def govrole(soup):
        """print funct:gov"""
        result = []
        for dep in soup.find_all('dep'):
            for dependent in dep.find_all('dependent'):
                word = dependent.get_text()
                if re.match(regex, word):
                    role = dep.attrs.get('type')
                    gov = dep.find_all('governor')
                    govword = gov[0].get_text()
                    # very messy here, sorry
                    if lemmatise is True:
                        if role in deptags:
                            thetag = deptags[role]
                        else:
                            thetag = None
                        if not thetag:
                            if lemmatag:
                                thetag = lemmatag
                            else:
                                thetag = 'v'
                        if word in wordlist:
                            word = wordlist[word]
                        govword = lmtzr.lemmatize(govword, thetag)
                    colsep = role + u':' + govword
                    result.append(colsep)
        return result


    def funct(soup):
        """"print functional role"""
        result = []
        for dep in soup.find_all('dep'):
            for dependent in dep.find_all('dependent'):
                word = dependent.get_text()
                if re.match(regex, word):
                    result.append(dep.attrs.get('type'))
        return result

    def depnum(soup):
        """print dependency number"""
        result = []
        for dep in soup.find_all('dep'):
            for dependent in dep.find_all('dependent'):
                word = dependent.get_text()
                if re.match(regex, word):
                    # get just the number
                    result.append(int(dependent.attrs.get('idx')))
        return result

    def depnum_reorder(results_list, output = 'results'):
        """reorder depnum results and/or generate totals list"""
        yearlist = [[unicode(i[0])] for i in results_list[0][1:]]
        #print yearlist
        totallist = [u'Totals']
        counts = []
        for entry in results_list: # for each depnum:
            depnum = entry[0]
            #print depnum
            count = sum(d[1] for d in entry[1:])
            #print count
            totallist.append([depnum, count])
        for year in yearlist:
            for entry in results_list:
                word = entry[0]
                data = entry[1:]
                #depnum_and_count = [word, sum([d[1] for d in data])] # sum for each depnum
                #totallist.append(depnum_and_count)
                for theyear, count in data:
                    if theyear == int(year[0]):
                        fixed_datum = [word, count]
                        year.append(fixed_datum)
        # this could be done more efficiently earlier:
        for year in yearlist:
            for entry in year[1:]:
                if entry[0] > 50:
                    year.remove(entry)
        for entry in totallist[1:]:
            if entry[0] > 50:
                    totallist.remove(entry)
        if output == 'results':
            return yearlist
        if output == 'totals':
            return totallist
        
#########################################################################
#########################################################################
#########################################################################

    regex = re.compile(query)
    sorted_dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path,d))]
    sorted_dirs.sort(key=int)
    if test:
        sorted_dirs = sorted_dirs[:test]
    allwords_list = []
    results_list = []
    main_totals = [u'Totals']
    num_dirs = len(sorted_dirs)
    all_files = []
    for index, d in enumerate(sorted_dirs):
        yearfinder = re.findall(r'[0-9]+', d)
        files = [f for f in os.listdir(os.path.join(path, d)) if f.endswith('.xml')]
        if test:
            files = files[:2000]
        #read_files = glob.glob(os.path.join(path, d, "*.xml"))
        all_files.append([d, files])
    total_files = len([item for sublist in all_files for item in sublist[1]])
        #for f in read_files:
    p = ProgressBar(total_files)
    c = 0
    for d, fileset in all_files:  
        result = []
        if test:
            fileset = fileset[:test * 2000]
        for f in fileset:
            p.animate(c, str(c) + '/' + str(total_files))
            c += 1
            with open(os.path.join(path, d, f), "rb") as text:
                data = text.read()
                just_good_deps = SoupStrainer('dependencies', type=dep_type)
                soup = BeautifulSoup(data, "lxml", parse_only=just_good_deps)
                if options == 'govrole':
                    result_from_file = govrole(soup)
                if options == 'funct':
                   result_from_file = funct(soup)
                if options == 'depnum':
                  result_from_file = depnum(soup)
            if result_from_file is not None:
                for entry in result_from_file:
                    result.append(entry)
            soup.decompose()
            soup = None
            data = None
            gc.collect()
        result.sort()
        main_totals.append([int(d), len(result)]) # should this move down for more accuracy?
        if only_count:
            continue
        if options != 'depnum':
            result = processwords(result)
        allwords_list.append(result)
        results_list.append(result)
    p.animate(total_files)
    if only_count:
        total = sum([i[1] for i in main_totals[1:]])
        main_totals.append([u'Total', total])
        outputnames = collections.namedtuple('interrogation', ['query', 'totals'])
        query_options = [query, options] 
        output = outputnames(query_options, main_totals)
        return output

    # flatten list
    allwords = [item for sublist in allwords_list for item in sublist]
    allwords.sort()
    unique_words = set(allwords)
    list_words = []
    for word in unique_words:
        list_words.append([word])


    # make dictionary of every subcorpus
    dicts = []
    p = ProgressBar(len(results_list))
    for index, subcorpus in enumerate(results_list):
        p.animate(index)
        subcorpus_name = sorted_dirs[index]
        dictionary = Counter(subcorpus)
        dicts.append(dictionary)
        for word in list_words:
            getval = dictionary[word[0]]
            word.append([int(subcorpus_name), getval])
    p.animate(len(results_list))
    # do totals (and keep them)
    if options == 'depnum':
        print list_words
        list_words.sort(key=lambda x: int(x[0]))
        main_totals = depnum_reorder(list_words, output = 'totals') 
        list_words = depnum_reorder(list_words, output = 'results') 
    else: 
        list_words.sort(key=lambda x: x[-1], reverse = True)
    print list_words
    for word in list_words:
        total = sum([i[1] for i in word[1:]])
        word.append([u'Total', total])

    #make results into named tuple
    outputnames = collections.namedtuple('interrogation', ['query', 'results', 'totals'])
    query_options = [query, options] 
    total = sum([i[1] for i in main_totals[1:]])
    main_totals.append([u'Total', total])
    output = outputnames(query_options, list_words, main_totals)
    if have_ipython:
        clear_output()
    return output
