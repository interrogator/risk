#!/usr/bin/python

#   Interrogating parsed corpora and plotting the results: dependencies
#   Author: Daniel McDonald

def dependencies(path, options, query, lemmatise = False, test = False, 
    titlefilter = False, lemmatag = False, dep_type = 'basic-dependencies', only_count = False):
    """Uses BeautifulSoup to make list of frequency counts in corpora.
    
    Interrogator investigates sets of Stanford dependencies for complex frequency information. 

    path: path to corpus as string
    options: 
        'funct': get functional label
        'depnum': get index of item within the clause
        'govrole': get role and governor, colon-separated
        query: regex to match
    Lemmatise: lemmatise results
    test: for development, go through only three subcorpora
    titlefilter: strip 'mr, mrs, dr' etc from proper noun strings
    dep_type: specify type of dependencies to search:
                    'basic-dependencies' * best lemmatisation
                    'collapsed-dependencies'
                    'collapsed-ccprocessed-dependencies'

    Note: subcorpora directory names must be numbers only.
    """
    import os
    from bs4 import BeautifulSoup, SoupStrainer
    import collections
    from collections import Counter
    import time
    from time import localtime, strftime
    import re
    import sys
    import string
    import codecs
    from string import digits
    import operator
    import glob
    from corpling_tools.progressbar import ProgressBar
    import gc
    try:
        from IPython.display import display, clear_output
        have_ipython = True
    except ImportError:
        have_ipython = False
    # define option regexes
    time = strftime("%H:%M:%S", localtime())
    try:
        from IPython.display import display, clear_output
        have_ipython = True
    except ImportError:
        have_ipython = False
    if lemmatise:
        from nltk.stem.wordnet import WordNetLemmatizer
        lmtzr=WordNetLemmatizer()
    strip_bad = re.compile('[^.a-zA-Z0-9/-:]', re.UNICODE)

    # welcomer
    if options == 'depnum':
        optiontext = 'Number only.'
    elif options == 'funct':
        optiontext = 'Functional role only.'
    elif options == 'govrole':
        optiontext = 'Role and governor.'
    else:
        raise ValueError("Option must be 'depnum', 'funct' or 'govrole'.")
    time = strftime("%H:%M:%S", localtime())
    print "%s: Begining corpus interrogation: %s\n          Query: %s\
    \n          %s\n          Interrogating corpus ... \n" % (time, path, query, optiontext)
    
    def processwords(list_of_matches):
        # encoding
        #matches = [unicode(match, 'utf-8', errors = 'ignore') for match in list_of_matches if type(match) != unicode]
        matches = [match for match in list_of_matches if type(match) == unicode]
        # remove bad characters

        matches = [re.sub(strip_bad, "", match) for match in matches]
        matches = filter(None, matches)
        #lowercasing
        lowered = []
        for match in matches:
            try:
                lowered.append(match.lower())
            except:
                lowered.append(match)
        #matches = [match.lower() for match in matches if any(c.isalpha() for c in match)]
        #matches.append([match for match in matches if not any(c.isalpha() for c in match)])
        if titlefilter:
            lowered = titlefilter(lowered)
        return lowered

    def titlefilter(list_of_matches):
        import re
        import nltk
        from data.dictionaries.titlewords import titlewords
        tokenised_list = [nltk.word_tokenize(i) for i in list_of_matches]
        output = []
        for result in tokenised_list:
            head = result[-1]
            non_head = result.index(head) # ???
            title_stripped = [token for token in result[:non_head] if token not in titlewords]
            title_stripped.append(head)
            str_result = ' '.join(title_stripped)
            output.append(str_result)
        return output


    def govrole(soup):
        """print funct:gov"""
        result = []
        for dep in soup.find_all('dep'):
            for dependent in dep.find_all('dependent'):
                word = dependent.get_text()
                if re.match(regex, word):
                    role = dep.attrs.get('type')
                    # fixing redundant loop here
                    gov = dep.find_all('governor')
                    govword = gov[0].get_text()
                    if lemmatise is True:
                        thetag = None
                        if role == 'amod':
                            thetag = 'n'
                        if role == 'nn':
                            thetag = 'n'
                        if role == 'rcmod':
                            thetag = 'n'
                        if role == 'dobj':
                            thetag = 'v'
                        if role == 'nsubj':
                            thetag = 'v'
                        if role == 'nsubjpass':
                            thetag = 'v'
                        if role == 'advmod':
                            thetag = 'v'
                        if role == 'iobj':
                            thetag = 'v'
                        if role == 'acomp':
                            thetag = 'v'
                        if role == 'cop':
                            thetag = 'v'
                        if role == 'advmod':
                            thetag = 'v'
                        if role == 'iobj':
                            thetag = 'v'
                        if role == 'xcomp':
                            thetag = 'v'
                        if role == 'ccomp':
                            thetag = 'v'
                        #if 'prep' in role:
                           #thetag = v
                        if govword == u'\'s':
                            govword = u'is'
                        if govword == u'\'re':
                            govword = u'are'
                        if govword == u'\'m':
                            govword = u'am'
                        #if govword == u'\'d':
                            #govword = u'had or would?'
                        #if govword == u'\'ll':
                            #govword = u'will or shall?'
                        if govword == u'n\'t':
                            govword = u'not'
                        if not thetag:
                            if lemmatag:
                                thetag = lemmatag
                            else:
                                thetag = 'v'
                        govword = lmtzr.lemmatize(govword, thetag)
                        colsep = role + u':' + govword
                        result.append(colsep)
        return result


    def funct(soup):
        """"print functional role"""
        result = []
        for dep in soup.find_all('dep'):
            for dependent in dep.find_all('dependent'):
                word = dependent.get_text()
                if re.match(regex, word):
                    result.append(dep.attrs.get('type'))
        return result

    def depnum(soup):
        """print dependency number"""
        result = []
        for dep in soup.find_all('dep'):
            for dependent in dep.find_all('dependent'):
                word = dependent.get_text()
                if re.match(regex, word):
                    # get just the number
                    result.append(int(dependent.attrs.get('idx')))
        return result
        
#########################################################################
#########################################################################
#########################################################################

    regex = re.compile(query)
    sorted_dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path,d))]
    sorted_dirs.sort(key=int)
    if test:
        sorted_dirs = sorted_dirs[:test]
    allwords_list = []
    results_list = []
    main_totals = [u'Totals']
    num_dirs = len(sorted_dirs)
    all_files = []
    for index, d in enumerate(sorted_dirs):
        yearfinder = re.findall(r'[0-9]+', d)
        files = [f for f in os.listdir(os.path.join(path, d)) if f.endswith('.xml')]
        if test:
            files = files[:100]
        #read_files = glob.glob(os.path.join(path, d, "*.xml"))
        all_files.append([d, files])
    total_files = len([item for sublist in all_files for item in sublist[1]])
        #for f in read_files:
    p = ProgressBar(total_files)
    c = 0
    for d, fileset in all_files:  
        result = []
        if test:
            fileset = fileset[:test * 100]
        for f in fileset:
            p.animate(c, str(c) + '/' + str(total_files))
            c += 1
            with open(os.path.join(path, d, f), "rb") as text:
                data = text.read()
                just_good_deps = SoupStrainer('dependencies', type=dep_type)
                soup = BeautifulSoup(data, parse_only=just_good_deps)
                if options == 'govrole':
                    result_from_file = govrole(soup)
                if options == 'funct':
                   result_from_file = funct(soup)
                if options == 'depnum':
                  result_from_file = depnum(soup)
            if result_from_file is not None:
                for entry in result_from_file:
                    result.append(entry)
            soup.decompose()
            soup = None
            data = None
            gc.collect()
        result.sort()
        main_totals.append([int(d), len(result)]) # should this move down for more accuracy?
        if only_count:
            continue
        if options != 'depnum':
            result = processwords(result)
        allwords_list.append(result)
        results_list.append(result)
    p.animate(total_files)
    if only_count:
        total = sum([i[1] for i in main_totals[1:]])
        main_totals.append([u'Total', total])
        outputnames = collections.namedtuple('interrogation', ['query', 'totals'])
        query_options = [query, options] 
        output = outputnames(query_options, main_totals)
        return output

    # flatten list
    allwords = [item for sublist in allwords_list for item in sublist]
    allwords.sort()
    unique_words = set(allwords)
    list_words = []
    for word in unique_words:
        list_words.append([word])


    # make dictionary of every subcorpus
    dicts = []
    p = ProgressBar(len(results_list))
    for index, subcorpus in enumerate(results_list):
        p.animate(index)
        subcorpus_name = sorted_dirs[index]
        dictionary = Counter(subcorpus)
        dicts.append(dictionary)
        for word in list_words:
            getval = dictionary[word[0]]
            word.append([int(subcorpus_name), getval])
    p.animate(len(results_list))
    # do totals (and keep them), then sort list by total
    for word in list_words:
        total = sum([i[1] for i in word[1:]])
        word.append([u'Total', total])
    list_words.sort(key=lambda x: x[-1], reverse = True)
    if options == 'depnum':
        list_words.sort(key=lambda x: int(x[0]))
    #make results into named tuple
    outputnames = collections.namedtuple('interrogation', ['query', 'results', 'totals'])
    query_options = [query, options] 
    total = sum([i[1] for i in main_totals[1:]])
    main_totals.append([u'Total', total])
    output = outputnames(query_options, list_words, main_totals)
    if have_ipython:
        clear_output()
    return output
