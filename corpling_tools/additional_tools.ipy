#!/usr/bin/ipython

#   Interrogating parsed corpora and plotting the results: additional resources
#   Author: Daniel McDonald

def surgeon(lst, criteria, remove = False, resort = False):
    """
    Add or remove from results by regex or index.

    criteria: if string, it is a regular expression to keep/remove by.
                 if list, a list of indices to remove keep remove by

    If regex, remember to add word boundaries!
    """
    import re
    newlist = []
    if type(criteria) == str:
        regexp = re.compile(criteria)
        for item in lst:
            if remove is True:
                if type(item) == str:
                    if not re.search(regexp, item):
                        newlist.append(item)
                else:
                    if not re.search(regexp, item[0]):
                        newlist.append(item)                        
            if remove is False:
                if type(item) == str:
                    if re.search(regexp, item):
                        newlist.append(item)
                else:
                    if re.search(regexp, item[0]):
                        newlist.append(item)     
    if type(criteria) == list:
        if remove is True:
            newlist = list(lst)
            backward_indices = sorted(criteria, reverse = True)
            for index in backward_indices:
                newlist.remove(newlist[index])
        if remove is False:
            for index in criteria:
                newlist.append(lst[index])
    if resort:
        if resort == 'alpha':
            resort(newlist, critera = 'alpha')
        else:
            resort(newlist, critera = 'total')
            # assume total sort
    return newlist

def quickview(lst, n = 50, topics = False):
    """See first n words of an interrogation.

    lst: interrogator() list
    n: number of results to view
    topics: for investigation of topic subcorpora"""
    if not topics:
        out = []
        for index, item in enumerate(lst[:n]):
            # if it's interrogator result
            if type(item) == list:
                word = item[0]
                index_and_word = [str(index), word]
                as_string = ': '.join(index_and_word)
                out.append(as_string)
            else:
                out.append(item)
        return out
    if topics:
        topics = [d for d in os.listdir(path)
        if os.path.isdir(os.path.join(path,d))
        and d != 'years']
        out = []
        for corpus in topics:
            subout = []
            out.append(corpus.upper())
            sublist = lst[topics.index(corpus)]
            subout = []
            for item in sublist[:n]:
                indexnum = sublist.index(item)
                word = item[0]
                index_and_word = [str(indexnum), word]
                as_string = ': '.join(index_and_word)
                subout.append(as_string)
            out.append(subout)
    return out

def topix_search(topic_subcorpora, options, query, **kwargs):
    """Interrogates each topic subcorpus."""
    results = [] # make empty list of results and totals
    totals = []
    for topic in topic_subcorpora: # for topic name
        print "Doing " + topic + " ..."
        topic_subcorpora_path = os.path.join(path,topic) # get topic path
        # interrogate with any arguments passed in:
        result = interrogator(topic_subcorpora_path, options, query, **kwargs)
        results.append(result.results) # add to results
        totals.append(result.totals)
    # now we should have 3x results and 3x totals, and a query
    outputnames = collections.namedtuple('interrogation', ['query', 'results', 'totals'])
    query_options = [query, options] 
    output = outputnames(query_options, results, totals)
    return results

def topix_plot(title, results, fract_of = False, **kwargs):
    """Plots results from subcorpus interrogation."""
    for topic in topic_subcorpora:
        newtitle = title + ' in ' + str(topic) + ' articles' # semi-automatic titles (!)
        if not fract_of: # if counting ratios/percentages, 
            plotter(newtitle, results.results[topic_subcorpora.index(topic)], **kwargs)
        else:
            plotter(newtitle, results[topic_subcorpora.index(topic)], 
                fract_of = fract_of.totals[topic_subcorpora.index(topic)], **kwargs)

def conc(corpus, query, random = False, window = 50, trees = False, csvmake = False): 
    """A concordancer for Tregex queries"""
    # add sorting?
    from random import randint
    import time
    from time import localtime, strftime
    import re
    from collections import defaultdict
    if csvmake:
        if os.path.isfile(csvmake):
            raise ValueError("CSV error: " + csvmake + " already exists in current directory. Move it, delete it, or change the name of the new .csv file.")

    def csvmaker(csvdata, sentences, csvmake):
        """Puts conc() results into tab-separated spreadsheet form"""
        #make data utf 8
        uc_data = []
        uc_sentences = []
        for line in csvdata:
            newline = []
            for part in line:
                newpart = unicode(part, 'utf-8')
                newline.append(newpart)
            uc_data.append(newline)
        for sentence in sentences:
            newsentence = unicode(sentence, 'utf-8')
            uc_sentences.append(newsentence)
        csv = []
        # make first line
        topline = query + '\nTab separated, with window (n=' + str(len(csvdata)) + '):\n'
        midline = '\n\n' + query + '\nEntire sentences (n=' + str(len(sentences)) + '):\n'
        # title then years for top row
        csv.append(topline)
        # for each word
        for entry in uc_data:
            sentence = '\t'.join(entry)
            csv.append(sentence)
        csv.append(midline)
        for entry in uc_sentences:
            csv.append(entry)
        csv = '\n'.join(csv)
        # write the csv file?
        try:
            fo=open(csvmake,"w")
        except IOError:
            print "Error writing CSV file."
        fo.write(csv.encode("UTF-8"))
        fo.close()
        time = strftime("%H:%M:%S", localtime())
        print time + ": " + csvmake + " written to currect directory."
                
    def list_duplicates(seq):
        tally = defaultdict(list)
        for i,item in enumerate(seq):
            tally[item].append(i)
        return ((key,locs) for key,locs in tally.items() 
                        if len(locs)>1)

        ##############################################################

    time = strftime("%H:%M:%S", localtime())
    print "\n" + time + ': Getting concordances for ' + corpus + ' ... \n          Query: ' + query + '\n'
    output = []
    tregex_command = 'sh ./tregex.sh \'' + query + '\' 2>&1'
    testpattern = !$tregex_command
    tregex_error = re.compile(r'^Error parsing expression')
    regex_error = re.compile(r'^Exception in thread')
    if re.match(tregex_error, testpattern[0]):
        tregex_error_output = "Error parsing Tregex expression. Check for balanced parentheses and boundary delimiters."
        raise ValueError(tregex_error_output)
    if re.match(regex_error, testpattern[0]):
        info = testpattern[0].split(':')
        index_of_error = re.findall(r'index [0-9]+', info[1])
        justnum = index_of_error[0].split('dex ')
        spaces = ' ' * int(justnum[1])
        remove_start = query.split('/', 1)
        remove_end = remove_start[1].split('/', -1)
        regex_error_output = 'Error parsing regex inside Tregex query:%s '\
        '. Best guess: \n%s\n%s^' % (str(info[1]), str(remove_end[0]), spaces)
        raise ValueError(regex_error_output)    
    if trees:
        options = '-s'
    else:
        options = '-t'
    if re.match(tregex_error, testpattern[0]):
        raise ValueError("Error parsing Tregex expression. "
            "Check for balanced parentheses and boundary delimiters.") 
    tregex_command = 'sh ./tregex.sh -o -w ' + options + ' \'' + query + '\' ' + corpus + ' 2>/dev/null | grep -vP \'^\s*$\''
    # replace bracket: '-LRB- ' and ' -RRB- ' ...

    allresults = !$tregex_command
    results = list(allresults)
    if csvmake: # this is not optimised at all!
        sentences = list(results)
    tregex_command = 'sh ./tregex.sh -o ' + options + ' \'' + query + '\' ' + corpus + ' 2>/dev/null | grep -vP \'^\s*$\''
    alltresults = !$tregex_command
    tresults = list(alltresults)
    zipped = zip(allresults, alltresults)
    all_dupes = []
    #for dup in sorted(list_duplicates(results)):
        #index_list = dup[1][1:] # the list of indices for each duplicate, minus the first one, which we still want.
        #for i in index_list:
            #all_dupes.append(i)
    #for i in sorted(all_dupes, reverse = True):
        #print tresults[i]
        #print results[i]
    #n = len(results)
    #for i in xrange(n):
        #print tresults[i]
        #print results[i]
    totalresults = len(zipped)
    if totalresults == 0:
        raise ValueError("No matches found, sorry. I wish there was more I could tell you.") 
    maximum = len(max(tresults, key=len)) # longest result in characters
    csvdata = []
    unique_results = []
    for result in zipped: 
        tree = result[0]
        pattern = result[1]
        regex = re.compile(r"(\b[^\s]{0,1}.{," + re.escape(str(window)) + r"})(\b" + 
            re.escape(pattern) + r"\b)(.{," + re.escape(str(window)) + r"}[^\s]\b)")
        search = re.findall(regex, tree)
        for result in search:
            unique_results.append(result)
    unique_results = set(sorted(unique_results)) # make unique
    for unique_result in unique_results:
        lstversion = list(unique_result)
        if len(lstversion) == 3:
            if csvmake:
                csvdata.append(lstversion)
            whitespace_first = window + 2 - len(lstversion[0])
            whitespace_second = maximum - len(lstversion[1])
            lstversion[0] = ' ' * whitespace_first + lstversion[0]
            lstversion[1] = lstversion[1] + ' ' * whitespace_second
            output.append(lstversion)
    formatted_output = []
    for row in output:
        formatted_output.append(" ".join(row))
    if csvmake:
        csvmaker(csvdata, sentences, csvmake)
    if not random:
        with_enum = []
        for index, row in enumerate(formatted_output):
            if len(str(index)) == 1:
                string_index = '  ' + str(index)
            if len(str(index)) == 2:
                string_index = ' ' + str(index)
            if len(str(index)) > 2:
                string_index = str(index)
            row = (string_index + ': ' + row)
            with_enum.append(row)
        for entry in with_enum:
            print entry
        return formatted_output
    if random:
        outnum = len(formatted_output)
        if random > outnum:
            random = outnum
        rand_out = []
        while len(rand_out) < random:
            randomnum = randint(0,outnum - 1)
            possible = formatted_output[randomnum]
            if possible not in rand_out:
                rand_out.append(possible)
        with_enum = []
        for index, row in enumerate(rand_out):
            if len(str(index)) == 1:
                string_index = '  ' + str(index)
            if len(str(index)) == 2:
                string_index = ' ' + str(index)
            if len(str(index)) > 2:
                string_index = str(index)
            row = (string_index + ': ' + row)
            with_enum.append(row)
        for entry in with_enum:
            print entry
        return rand_out

def tally(lst, indices):
    """Display total occurrences of a result"""

    # this tool doesn't do a whole lot, now that totals are found during interrogation.
    output = []
    if type(indices) == int:
        item_of_interest = lst[indices]
        word = item_of_interest[0]
        total = item_of_interest[-1][-1]
        string = str(indices) + ': ' + str(word) + ': ' + str(total) + ' total occurrences.'
        output.append(string)
    if type(indices) == list:
        for index in indices:
            item_of_interest = lst[index]
            word = item_of_interest[0]
            total = item_of_interest[-1][-1]
            string = str(index) + ': ' + str(word) + ': ' + str(total) + ' total occurrences.'
            output.append(string)
    if type(indices) == str:
        if indices == 'all':
            for item in lst:
                word = item[0]
                total = item[-1][-1]
                string = str(lst.index(item)) + ': ' + str(word) + ': ' + str(total) + ' total occurrences.'
                output.append(string)
        else: # if regex
            import re
            regex = re.compile(indices)    
            for item in lst:
                if re.search(regex, item[0]):
                    word = item[0]
                    total = item[-1][-1]
                    string = str(lst.index(item)) + ': ' + str(word) + ': ' + str(total) + ' total occurrences.'
                    output.append(string)
                #else:
                    #raise ValueError("No matches found. Sorry")
    return output

def merger(lst, indices_to_merge, newname = False, printmerge = True):
    """Merges result items by their index

    lst: list to work on
    indices_to_merge: list of result indexes
    newname = if str, this becomes the new name
                        if int, the item indexed with that int becomes newname
                        if False, most common item becomes newname"""
    import re
    tomerge = []
    oldlist_copy = list(lst)
    # if regex, get list of indices for mwatches
    if type(indices_to_merge) ==  str:
        forwards_index = []
        regex = re.compile(indices_to_merge)
        for entry in oldlist_copy:
            if re.search(regex, str(entry[0])):
                forwards_index.append(oldlist_copy.index(entry))
        backward_indices = sorted(forwards_index, reverse = True)
    #if indices, ensure results are sorted
    if type(indices_to_merge) == list:
        forwards_index = list(indices_to_merge)
        backward_indices = sorted(indices_to_merge, reverse = True)
    # remove old entries
    for index in backward_indices:
        oldlist_copy.remove(oldlist_copy[index])
    # add matching entries to tomerge
    for index in forwards_index:
        tomerge.append(lst[index])
    # make list of words that are being merged
    toprint = []
    for entry in tomerge:
        toprint.append(str(entry[0]))
    string_to_print = '\n'.join(toprint)
    if printmerge:
        print 'Merging the following entries:\n' + string_to_print
    combined = zip(*[l for l in tomerge])
    merged = []
    # get the new word or use the first entry
    if type(newname) == int:
        getnewname= lst[newname]
        the_newname = getnewname[0]
    elif type(newname) == str:
        the_newname = unicode(newname)
    elif type(newname) == unicode:
        the_newname = newname
    else:
        duplicated_word = combined[0]
        the_newname = duplicated_word[0]
    merged.append(the_newname)
    for tup in combined[1:]: # for each tuple of combined years and counts
        getyearfrom = tup[0]
        year = getyearfrom[0]
        counts = []
        for bit in tup:
            counts.append(bit[1])
        total = sum(counts)
        goodtup = [year, total]
        merged.append(goodtup)
    output = []
    if type(indices_to_merge) == list:
        forwards_index = sorted(indices_to_merge)
    # could make a for item in forwards_index, put word in first place, put dummies in others...
    first_index = forwards_index[0]
    for entry in oldlist_copy[:first_index]:
        output.append(entry)
    output.append(merged)
    for entry in oldlist_copy[first_index:]:
         output.append(entry)
    return output

def searchtree(tree, query):
    "Searches a tree with Tregex and returns matching terminals"
    ! echo "$tree" > "tmp.tree"
    tregex_command = 'sh ./tregex.sh -o -t \'' + query + '\' tmp.tree 2>/dev/null | grep -vP \'^\s*$\''
    result = !$tregex_command
    ! rm "tmp.tree"
    return result

def quicktree(tree):
    """Return a visual representation of a parse tree in IPython"""
    from nltk import Tree
    from nltk.draw.util import CanvasFrame
    from nltk.draw import TreeWidget
    from IPython.display import display
    from IPython.display import Image
    parsed = Tree.fromstring(tree)
    cf = CanvasFrame()
    tc = TreeWidget(cf.canvas(),parsed)
    cf.add_widget(tc,10,10) # (10,10) offsets
    cf.print_to_file('tree.ps')
    cf.destroy()
    ! convert tree.ps tree.png
    ! rm tree.ps
    return Image(filename='tree.png')
    ! rm tree.png

def table(data_to_table, allresults = False, maxresults = 50):
    """Outputs a table from interrogator or CSV results.

    if allresults, show all results table, rather than just plotted results"""
    import pandas
    import re
    import os
    from StringIO import StringIO
    if type(data_to_table) == str:
        f = open(data_to_table)
        raw = f.read()
        #raw = os.linesep.join([s for s in raw.splitlines() if s])
        plotted_results, all_results =  raw.split('All results:')
        if not allresults:
            lines = plotted_results.split('\n')
            print str(lines[1])
            data = '\n'.join([line for line in lines if line.strip()][2:])
        if allresults:
            lines = all_results.split('\n')
            print str(lines[1])
            lines = lines[:maxresults + 3]
            data = '\n'.join([line for line in lines if line.strip()][1:])
    elif type(data_to_table) == list:
        csv = []
        if type(data_to_table[0]) == str or type(data_to_table[0]) == unicode:
            wrapped = [list(data_to_table)]
        else:
            wrapped = list(data_to_table)
        regex = re.compile('(?i)total')
        if re.match(regex, wrapped[-1][0]):
            total_present = True
        else:
            total_present = False
        years = [str(year) for year, count in wrapped[0][1:]]
        # uncomment below to make total column
        topline = ',' + ','.join(years) # + ',total'
        csv.append(topline)
        data = []
        for entry in wrapped[:maxresults]:
            word = entry[0]
            total = sum([count for year, count in entry[1:]])
            counts = [str(count) for year, count in entry[1:]]
            # uncomment below to make total column
            dataline = str(word) + ',' + ','.join(counts) # + ',' + str(total)
            csv.append(dataline)
        # table it with pandas
        data = '\n'.join(csv)
    tab = pandas.read_csv(StringIO(data), index_col = 0, engine='python')
    return tab


def keywords(data, dictionary = 'years.p', **kwargs):
    """Feed this a csv file generated with conc() and get its keywords"""
    #import sys
    #sys.path.insert(0, 'spindle-code-master/keywords')
    % run corpling_tools/keywords.ipy
    import re
    # turn all sentences into long string
    f = open(data)
    raw = f.read()
    bad, good = re.compile(r'Entire sentences \(n=[0-9]+\):').split(raw)
    keywords, ngrams = keywords_and_ngrams(good, dictionary = dictionary, **kwargs)
    keywords_list_version = []
    for index, item in enumerate(keywords):
        aslist = [index, item[0], item[1]]
        keywords_list_version.append(aslist)
    ngrams_list_version = []
    for index, item in enumerate(ngrams):
        joined_ngram = ' '.join(item[0])
        aslist = [index, joined_ngram, item[1]]
        ngrams_list_version.append(aslist)    
    return keywords_list_version, ngrams_list_version

# keywords, ngrams = keywords('test.csv', nBigrams = 4)
# print ngrams

def collocates(data, nbest = 30, window = 5):
    """Feed this a csv file generated with conc() and get its collocations"""
    import nltk
    from nltk import collocations
    from nltk.collocations import BigramCollocationFinder
    import re
    f = open(data)
    raw = f.read()
    bad, good = re.compile(r'Entire sentences \(n=[0-9]+\):').split(raw)
    good = unicode(good.lower(), 'utf-8', errors = 'ignore')
    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')
    sents = sent_tokenizer.tokenize(good)
    tokenized_sents = [nltk.word_tokenize(i) for i in sents]
    allwords = []
    # for each sentence,
    for sent in tokenized_sents:
    # for each word,
        for word in sent:
        # make a list of all words
            allwords.append(word)
    bigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_words(allwords, window_size=window)
    ignored_words = nltk.corpus.stopwords.words('english')
    # anything containing letter or number
    regex = r'[A-Za-z0-9]'
    # the n't token
    nonot = r'n\'t'
    # lots of conditions!
    finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() \
        in ignored_words or not re.match(regex, w) or re.match(nonot, w))
    finder.apply_freq_filter(2)
    results = sorted(finder.nbest(bigram_measures.raw_freq, nbest))
    listversion = []
    for index, thecollocation in enumerate(results):
        aslist = [index, thecollocation[0], thecollocation[1]]
        listversion.append(aslist)
    return listversion

def report_display():
    """Displays/downloads the risk report, depending on your browser settings"""
    class PDF(object):
        def __init__(self, pdf, size=(200,200)):
            self.pdf = pdf
            self.size = size
        def _repr_html_(self):
            return '<iframe src={0} width={1[0]} height={1[1]}></iframe>'.format(self.pdf, self.size)
        def _repr_latex_(self):
            return r'\includegraphics[width=1.0\textwidth]{{{0}}}'.format(self.pdf)
    return PDF('report/risk_report.pdf',size=(800,650))

# colls = collocates('test.csv')
# print colls

def parsetree(tree):
    """Parse a sentence and return a visual representation in IPython"""
    from nltk import Tree
    from nltk.draw.util import CanvasFrame
    from nltk.draw import TreeWidget
    from stat_parser import Parser
    from IPython.display import display
    from IPython.display import Image
    parser = Parser()
    parsed = parser.parse(sentence)
    cf = CanvasFrame()
    tc = TreeWidget(cf.canvas(),parsed)
    cf.add_widget(tc,10,10) # (10,10) offsets
    cf.print_to_file('tree.ps')
    cf.destroy()
    ! convert tree.ps tree.png
    ! rm tree.ps
    return Image(filename='tree.png')
    ! rm tree.png

def resort(lst, criteria = 'total', reverse = True):
    """Re-sort interrogation results alphabetically or by total"""
    # consider adding this within surgeon and merger
    from operator import itemgetter # for more complex sorting
    to_reorder = list(lst)
    if criteria == 'total':
        for item in to_reorder:
            total = sum([t[1] for t in item[1:]])
            item.append(total)
        sorted = sorted(to_reorder, key=lambda x: x[-1], reverse = reverse)
        for item in sorted:
            item.pop()
    if criteria == 'alpha':
        sorted = to_reorder.sort(key=lambda x: x[0])
    return sorted

def multiquery(corpus, query):
    """Creates a named tuple for a list of named queries to count

    Pass in something like:

    [[u'NPs', r'NP'], [u'VPs', r'VP']]"""

    import collections
    results = []
    for name, pattern in query:
        result = interrogator(corpus, '-C', pattern)
        result.totals[0] = name # rename count
        results.append(result.totals)
    totals = merger(results, r'.*', newname = 'Totals', printmerge = False)
    outputnames = collections.namedtuple('interrogation', ['query', 'results', 'totals'])
    output = outputnames(query, results, totals[0])
    return output
