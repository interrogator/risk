#!/usr/bin/ipython

#   Building parsed corpora for discourse analysis
#   Author: Daniel McDonald

# tool list:
# get text from file
# structure text by integer
# spelling normalisation
# parse files

def correctspelling(path, newpath):
    import enchant
    import codecs
    import os
    """Feed this function an unstructured corpus and get a version with corrected spelling"""
    subdirs = [d for d in os.listdir(path) if os.path.isdir(d)]
    for subdir in subdirs:
        txtFiles = [f for f in os.listdir(os.path.join(path,subdir)) if f.endswith(".txt")]
        print 'Doing ' + subdir + ' ...'
        for txtFile in txtFiles: 
            d = enchant.Dict("en_UK")
            try:
                f = codecs.open(os.path.join(path,subdir,txtFile), "r", "utf-8")
            except IOError:
                print "Error reading the file, right filepath?"
                return
            textdata = f.read()
            textdata = unicode(textdata, 'utf-8')
            mispelled = [] # empty list. Gonna put mispelled words in here
            words = textdata.split()
            for word in words:
                # if spell check failed and the word is also not in
                # our mis-spelled list already, then add the word
                if d.check(word) == False and word not in mispelled:
                    mispelled.append(word)
            # print mispelled
            for mspellword in mispelled:
                mspellword_withboundaries = '\b' + str(mspellword) + '\b'
                #get suggestions
                suggestions=d.suggest(mspellword)
                #make sure we actually got some
                if len(suggestions) > 0:
                    # pick the first one
                    picksuggestion=suggestions[0]
                    picksuggestion_withboundaries = '\b' + str(picksuggestion) + '\b'

                textdata = textdata.replace(mspellword_withboundaries,picksuggestion_withboundaries)
            try:
                if not os.path.exists(fraser_corpus_corrected):
                    os.makedirs(fraser_corpus_corrected)
                fo=open(os.path.join(newpath, txtFile), "w")
            except IOError:
                print "Error"
                return 
            fo.write(textdata.encode("UTF-8"))
            fo.close()
    return



def dictmaker(path, dictname, dictpath = 'data/dictionaries'):
    """makes a pickle wordlist named dictname in dictpath"""
    import os
    import pickle
    import re
    import nltk
    from time import localtime, strftime
    from StringIO import StringIO
    import shutil
    from collections import Counter
    from corpling_tools.progressbar import ProgressBar
  
    try:
        from IPython.display import display, clear_output
        have_ipython = True
    except ImportError:
        have_ipython = False
    sorted_dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path,d))]
    sorted_dirs.sort(key=int)
    try:
        if not os.path.exists(dictpath):
            os.makedirs(dictpath)
    except IOError:
        print "Error making " + dictpath + "/ directory."
    if os.path.isfile(os.path.join(dictpath, dictname)):
        raise ValueError(os.path.join(dictpath, dictname) + " already exists. Delete it or use a new filename.")
    time = strftime("%H:%M:%S", localtime())
    print time + ': Getting leaves from trees ... \n'
    p = ProgressBar(len(sorted_dirs))
    all_results = []
    for index, d in enumerate(sorted_dirs):
        p.animate(index + 1)
        tregex_command = 'sh ./tregex.sh -o -w \'ROOT < __\' ' + os.path.join(path, d) + ' 2>/dev/null | grep -vP \'^\s*$\''
        results = !$tregex_command
        for line in results:
            all_results.append(line)
    time = strftime("%H:%M:%S", localtime())
    print '\n\n' + time + ': Tokenising ' + str(len(all_results)) + ' lines ... \n'
    all_results = '\n'.join(all_results)
    text = unicode(all_results.lower(), 'utf-8', errors = 'ignore')
    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')
    sents = sent_tokenizer.tokenize(text)
    
    def tokens():
        p = ProgressBar(len(sents))
        for index, sent in enumerate(sents):
            p.animate(index + 1)
            yield nltk.word_tokenize(sent)

    tokenisation = tokens() # create a generator
    allwords = []
    allwords.append([item for sublist in tokenisation for item in sublist])
    # sort allwords
    allwords = allwords[0]
    allwords = sorted(allwords)
    time = strftime("%H:%M:%S", localtime())
    print '\n' + time + ': Counting ' + str(len(allwords)) + ' words ... \n'
    # make a dict
    dictionary = Counter(allwords)
    with open(os.path.join(dictpath, dictname), 'wb') as handle:
        pickle.dump(dictionary, handle)
    time = strftime("%H:%M:%S", localtime())
    print time + ': Done! ' + os.path.join(dictpath, dictname) + ' created' 
