#!/usr/bin/ipython

#   Building parsed corpora for discourse analysis
#   Author: Daniel McDonald

# tool list:
# get text from file
# structure text by integer
# spelling normalisation
# parse files

def correctspelling(path, newpath):
    import enchant
    import codecs
    import os
    """Feed this function an unstructured corpus and get a version with corrected spelling"""
    subdirs = [d for d in os.listdir(path) if os.path.isdir(d)]
    for subdir in subdirs:
        txtFiles = [f for f in os.listdir(os.path.join(path,subdir)) if f.endswith(".txt")]
        print 'Doing ' + subdir + ' ...'
        for txtFile in txtFiles: 
            d = enchant.Dict("en_UK")
            try:
                f = codecs.open(os.path.join(path,subdir,txtFile), "r", "utf-8")
            except IOError:
                print "Error reading the file, right filepath?"
                return
            textdata = f.read()
            textdata = unicode(textdata, 'utf-8')
            mispelled = [] # empty list. Gonna put mispelled words in here
            words = textdata.split()
            for word in words:
                # if spell check failed and the word is also not in
                # our mis-spelled list already, then add the word
                if d.check(word) == False and word not in mispelled:
                    mispelled.append(word)
            # print mispelled
            for mspellword in mispelled:
                mspellword_withboundaries = '\b' + str(mspellword) + '\b'
                #get suggestions
                suggestions=d.suggest(mspellword)
                #make sure we actually got some
                if len(suggestions) > 0:
                    # pick the first one
                    picksuggestion=suggestions[0]
                    picksuggestion_withboundaries = '\b' + str(picksuggestion) + '\b'

                textdata = textdata.replace(mspellword_withboundaries,picksuggestion_withboundaries)
            try:
                if not os.path.exists(fraser_corpus_corrected):
                    os.makedirs(fraser_corpus_corrected)
                fo=open(os.path.join(newpath, txtFile), "w")
            except IOError:
                print "Error"
                return 
            fo.write(textdata.encode("UTF-8"))
            fo.close()
    return



def dictmaker(path, dictname, dictpath = 'data/dictionaries'):
    """makes a pickle wordlist named dictname in dictpath"""
    import os
    import pickle
    import re
    import nltk
    from time import localtime, strftime
    from StringIO import StringIO
    import shutil
    from collections import Counter

    # suppress output from conc

    # progress bar

    class ProgressBar:
        def __init__(self, iterations):
            self.iterations = iterations
            self.prog_bar = '[]'
            self.fill_char = '*'
            self.width = 60
            self.__update_amount(0)
            if have_ipython:
                self.animate = self.animate_ipython
            else:
                self.animate = self.animate_noipython
    
        def animate_ipython(self, iter, dirname = None):
            print '\r', self,
            sys.stdout.flush()
            if dirname:
                self.update_iteration(iter + 1, dirname)
            else:
                self.update_iteration(iter + 1, dirname)
    
        def update_iteration(self, elapsed_iter, dirname = None):
            self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0, dirname)
            self.prog_bar += ' ' # + ' %d of %s complete' % (elapsed_iter, self.iterations)
    
        def __update_amount(self, new_amount, dirname = None):
            percent_done = int(round((new_amount / 100.0) * 100.0))
            all_full = self.width - 2
            num_hashes = int(round((percent_done / 100.0) * all_full))
            time = strftime("%H:%M:%S", localtime())
            self.prog_bar = time + ': [' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'
            pct_place = (len(self.prog_bar) // 2) + (len(str(percent_done)) // 2)
            if dirname:
                pct_string = '%d%% ' % percent_done + '(' + dirname + ')'
            else:
                pct_string = '%d%%' % percent_done # could pass dirname here!
            self.prog_bar = self.prog_bar[0:pct_place] + \
               (pct_string + self.prog_bar[pct_place + len(pct_string):])
    
        def __str__(self):
            return str(self.prog_bar)
  
    try:
        from IPython.display import display, clear_output
        have_ipython = True
    except ImportError:
        have_ipython = False
    sorted_dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path,d))]
    sorted_dirs.sort(key=int)
    try:
        if not os.path.exists(dictpath):
            os.makedirs(dictpath)
    except IOError:
        print "Error making " + dictpath + "/ directory."
    if os.path.isfile(os.path.join(dictpath, dictname)):
        raise ValueError(os.path.join(dictpath, dictname) + " already exists. Delete it or use a new filename.")
    time = strftime("%H:%M:%S", localtime())
    print time + ': Concordancing ... \n'
    p = ProgressBar(len(sorted_dirs))
    all_results = []
    for index, d in enumerate(sorted_dirs):
        p.animate(index + 1)
        tregex_command = 'sh ./tregex.sh -o -w \'ROOT < __\' ' + os.path.join(path, d) + ' 2>/dev/null | grep -vP \'^\s*$\''
        results = !$tregex_command
        for line in results:
            all_results.append(line)
    time = strftime("%H:%M:%S", localtime())
    print '\n\n' + time + ': Tokenising ' + str(len(all_results)) + ' lines ... \n'
    all_results = '\n'.join(all_results)
    text = unicode(all_results.lower(), 'utf-8', errors = 'ignore')
    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')
    sents = sent_tokenizer.tokenize(text)
    tokenized_sents = [nltk.word_tokenize(i) for i in sents]
    # flatten  allwords
    allwords = [item for sublist in tokenized_sents for item in sublist]
    # sort allwords
    allwords = sorted(allwords)
    time = strftime("%H:%M:%S", localtime())
    print time + ': Counting ' + str(len(allwords)) + ' words ... \n'
    # make a dict
    dictionary = Counter(allwords)
    with open(os.path.join(dictpath, dictname), 'wb') as handle:
        pickle.dump(dictionary, handle)
    time = strftime("%H:%M:%S", localtime())
    print time + ': Done! ' + dictname + ' created in ' + dictpath + '/'

