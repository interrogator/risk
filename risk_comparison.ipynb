{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of U.S. newspapers' longitudinal use of *risk words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Daniel McDonald](mailto:mcdonaldd@unimelb.edu.au?Subject=IPython%20NYT%20risk%20project), [Jens Zinn](mailto:jzinn@unimelb.edu.au?Subject=IPython%20NYT%20risk%20project), University of Melbourne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this notebook, we build on a preliminary investigation of risk words in *The New York Times*, looking at five additional mainstream U.S. newspapers. See our [GitHub repository](https://www.github.com/interrogator/risk), [IPython Notebook](https://github.com/interrogator/risk/blob/master/risk.ipynb), or [project report](https://raw.githubusercontent.com/interrogator/risk/master/risk_report.pdf) for more information. The theoretical underpinnings of our work, for example, are outlined in detail in the report.\n",
    "\n",
    "> This Notebook assumes its reader has basic familiarity with **Python** and **key linguistic concepts**, such as word class, syntax, and lemma.\n",
    "\n",
    "## Introduction: sociology, linguistics, code\n",
    "\n",
    "Sociologists such as Ulrich Beck and Anthony Giddens have characterised late modernity as *a risk society*, where risk plays an increasingly central role in both institutional structures and everyday life.\n",
    "\n",
    "Functional linguists are interested in mapping the ways in which words and wordings come together to both construct and represent particular discourses and ideologies. Over the past fifty years, linguists working within functional traditions have mapped out in detail how language is employed by its users as a resource for negotiating interpersonal relationships and for representing doings and happenings in the world, or in consciousness.\n",
    "\n",
    "Assuming that an increasing salience of *risk* in society will at least partly be reflected in the ways in which risk is discussed, it should therefore be possible to use real-world communication about risk to empirically examine sociological claims.\n",
    "\n",
    "A number of technological developments make it possible to investigate risk semantics on a large scale:\n",
    "\n",
    "1. Large, well-organised digital collections of news articles\n",
    "2. Tools for annotating digital text with linguistic information, such as word classes and grammatical structure\n",
    "3. Programming languages, libraries and modules that can extract useful information from these annotations\n",
    "\n",
    "That said, the use of corpus linguistics for discourse analysis is a relatively recent development, with available tools and methods still somewhat behind the state of the art resources available in computaional linguistics, natural language processing, etc.\n",
    "\n",
    "Accordingly, we use [*corpkit*](https://www.github.com/interrogator/corpkit), a purpose-built Python module for interrogating parsed corpora. The tool is also available as a graphical application, documented and downloadable [here](http://interrogator.github.io/corpkit/). Though our investigation could be performed using the graphical interface, we have instead opted for the command-line tools, which offer more flexibility, especially when working with multiple corpora simultaneously.\n",
    "\n",
    "## Aim of the investigation\n",
    "\n",
    "Our main interest was in determining **whether findings from our investigation of the NYT could be generalised to other major U.S. newspapers.**\n",
    "\n",
    "This investigation makes it possible to describe, and hopefully explain, how risk words have behaved longitudinally in mainstream U.S. newspapers. Later work will connect these findings more explicitly to sociological claims.\n",
    "\n",
    "## Data\n",
    "\n",
    "Based on readership, location and digital availability, we selected the following six newspapers.\n",
    "\n",
    "    1. The New York Times\n",
    "    2. The Wall Street Journal\n",
    "    3. The Tampa Bay Times\n",
    "    4. USA Today\n",
    "    5. Chicago Tribune\n",
    "    6. The Washington Post\n",
    "\n",
    "## Corpus building\n",
    "\n",
    "To do this, we used ProQuest to grab any articles in these newspapers from 1987-2015 that contained a risk word (defined by regular expression as `(?i)\\brisk` ). This left us with over 500,000 articles!\n",
    "\n",
    "Paragraphs with risk words were extracted and parsed using *Stanford CoreNLP*. Given the computationally intensive nature of parsing, we relied on high-performance computing resources from the University of Melbourne to run an *embarrassingly parallel* parsing script.\n",
    "\n",
    "The result of this process was six folders/corpora (one for each newspaper), which each containing annual subcorpora, containing CoreNLP `XML` annotation of risk paragraphs.\n",
    "\n",
    "## Analytical approach\n",
    "\n",
    "Once we had the data annotated, the challenges were to:\n",
    "\n",
    "1. Search the data to find meaningful information about the way risk words behave\n",
    "2. Turn raw findings into useful visualisations, descriptions and explanations\n",
    "\n",
    "Computationally, we used *corpkit* to search the constituency and dependency parses for lexicogrammatical and discourse-semantic sites of change. Theoretically, we used concepts from *systemic functional linguistics*, which articulates the ways in which levels of linguistic abstraction are related, and the ways in which different kinds of meaning are realised in grammar and wording.\n",
    "\n",
    "## Key limitations\n",
    "\n",
    "There are a number of fairly serious limitations that should be acknowledged upfront.\n",
    "\n",
    "First, we are looking only at mainstream U.S. newspaper articles. Our findings do not reflect society generally, in the USA or otherwise: risk words likely behave very differently in different text types.\n",
    "\n",
    "Computationally, we must acknowledge that parser accuracy may be an issue.\n",
    "\n",
    "In terms of linguistic theory, relevant concepts from systemic functional linguistics cannot be operationalised fully, given differences between the systemic-functional grammar and the grammars with which texts were annotated.\n",
    "\n",
    "Finally, **we're not really investigating the concept of risk, but only risk words**. Risk as a concept can be construed without the word being present (*\"They had to decide which was safer ... \"*). We chose to focus only on risk words because there is less room for ambiguity about whether or not a risk is being construed, and to reduce our dataset to a manageable size. To parse all text from six newspapers over a 30 year period would perhaps be the largest amount of data ever parsed for a single academic project, requiring massive dedicated resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "First, we need to import the corpkit module, as well as *pandas*, which can help us manipulate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# import corpkit\n",
    "from corpkit import interrogator, editor, plotter, conc\n",
    "\n",
    "# some wordlists we'll use later\n",
    "from dictionaries.process_types import processes\n",
    "from dictionaries.wordlists import wordlists\n",
    "from dictionaries.roles import roles\n",
    "\n",
    "# for editing/combining results:\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to set paths to our corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyt = 'data/NYT'\n",
    "wsj = 'data/WSJ'\n",
    "wap = 'data/WAP'\n",
    "cht = 'data/CHT'\n",
    "ust = 'data/UST'\n",
    "tbt = 'data/TBT'\n",
    "all_corpora = [nyt, wsj, wap, cht, ust, tbt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's set our very simple regular expression for *risk words*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "riskword = r'(?i)\\brisk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ready? OK, let's interrogate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training\n",
    "\n",
    "*corpkit* is essentially four functions and some wordlists. Here are the functions and their main arguments:\n",
    "\n",
    "> `interrogator(corpus, searchtype, query, **optional_args)` searches corpora and tabulates raw frequency results\n",
    "\n",
    "> `editor(result_to_edit, operation, denominator, **optional_args)` edits these results, by merging, skipping, renaming, sorting, keywording, summing, etc.\n",
    "\n",
    "> `plotter(title, results_to_plot, **_optional_args)` allows us to visualise `interrogator()` and `editor()` results\n",
    "\n",
    "> `conc(subcorpus, searchtype, query, **optional_args)` concordances corpora. It can help show results in more detail.\n",
    "\n",
    "The wordlists can be accessed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"'d\", u\"'m\", u\"'re\", u\"'s\", u\"'ve\", u'am', u'appear', u'appeared', u'appearing', u'appears', u'are', u'be', u'became', u'become', u'becomes', u'becoming', u'been', u'being', u'feel', u'feeling', u'feels', u'felt', u'had', u'has', u'have', u'having', u'is', u'look', u'looked', u'looking', u'looks', u'seem', u'seemed', u'seeming', u'seems', u'smell', u'smelled', u'smelling', u'smells', u'smelt', u'sound', u'sounded', u'sounding', u'sounds', u'was', u'were']\n"
     ]
    }
   ],
   "source": [
    "print processes.relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'all', u'anotha', u'another', u'any', u'any-and-all', u'atta', u'both', u'certain', u'couple', u'dat', u'dem', u'dis', u'each', u'either', u'enough', u'enuf', u'enuff', u'every', u'few', u'fewer', u'fewest', u'her', u'hes', u'his', u'its', u'last', u'least', u'many', u'more', u'most', u'much', u'muchee', u'my', u'neither', u'nil', u'no', u'none', u'other', u'our', u'overmuch', u'owne', u'plenty', u'quodque', u'several', u'some', u'such', u'sufficient', u'that', u'their', u'them', u'these', u'they', u'thilk', u'thine', u'this', u'those', u'thy', u'umpteen', u'us', u'various', u'wat', u'we', u'what', u'whatever', u'which', u'whichever', u'yonder', u'you', u'your']\n"
     ]
    }
   ],
   "source": [
    "print wordlists.determiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ccomp', 'cop', 'prt', 'root']\n"
     ]
    }
   ],
   "source": [
    "print roles.process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordlists can be used as queries, or as criteria to match during editing.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "So, the first thing we'll need to do is get some basic stuff:\n",
    "\n",
    "1. The number of words in each corpus\n",
    "2. The number of risk words in each corpus\n",
    "3. The part of speech tags for risk words in each corpus\n",
    "4. The word class of risk words\n",
    "\n",
    "The basic syntax for using `interrogator()` is to provide:\n",
    "\n",
    "1. a path to a corpus, or a list of paths\n",
    "2. a search type (there are a lot!)\n",
    "3. a search query\n",
    "4. optional arguments, for saving results, lemmatising, etc.\n",
    "\n",
    "When `interrogator()` gets a single string as its first argument, it treats the string as a path to a corpus, and outputs an object with `query`, `results` and `totals` attributes. When it receives a list of strings, it understands that there are multiple corpora to search. Using parallel processing, it searches each one, and returns a `dict` object with paths as keys and named tuple objects as values.\n",
    "\n",
    "Note that our risk regular expression needs to be inside `\"/ /\"` boundaries, because here we're using [Tregex](http://nlp.stanford.edu/manning/courses/ling289/Tregex.html) syntax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a named tuple with results, totals and query:\n",
    "allwords = interrogator(all_corpora, 'count', 'any', quicksave = '6_allwords')\n",
    "\n",
    "# returns a dict with paths as keys and named tuples as values:\n",
    "riskwords = interrogator(all_corpora, 'words', '/%s/' % riskword, quicksave = '6_riskwords')\n",
    "riskclasses = interrogator(all_corpora, 'pos', '/%s/' % riskword, quicksave = '6_riskclasses')\n",
    "\n",
    "# the lemmatise option turns words to their stem \n",
    "# form, but turns pos tags to their major word class\n",
    "risktags = interrogator(all_corpora, 'pos', '/%s/' % riskword, \n",
    "                        lemmatise = True, quicksave = '6_risktags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using dependencies\n",
    "\n",
    "We can also look at risk words by dependency role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "riskfunct = interrogator(all_corpora, 'function', riskword, quicksave = '6_riskfunct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below converts the functions to systemic labels using `editor()`. `editor()` can receive either a results attribute as its main input, or a `dict` object outputted by `interrogator()`. In the case of the latter, it outputs another `dict` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merges = {'Participant': roles.participant,\n",
    "          'Process': roles.process,\n",
    "          'Modifier': roles.modifier}\n",
    "\n",
    "sysfunc = editor(riskfunct, '%', 'self', merge_entries = merges, just_entries = merges.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot a single newspaper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotter('Systemic role of risk words in the NYT', sysfunc['wsj'].results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can look at the behaviour of a given role in every paper. To do this, let's write a simple function that extracts an entry from each result and concatenates the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entry_across_corpora(result_dict, entry_name):\n",
    "    import pandas as pd\n",
    "    res = []\n",
    "    # for each corpus name and data\n",
    "    for k, v in sorted(result_dict.items()):\n",
    "        # grab the process result for each paper\n",
    "        column = v.results[entry_name]\n",
    "        # rename it to the corpus name\n",
    "        column.name = k\n",
    "        # append to a list\n",
    "        res.append(column)\n",
    "    # concatenate and return\n",
    "    return pd.concat(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proc = entry_across_corpora(sysfunc, 'Process')\n",
    "plotter('Frequency of risk processes by newspaper', proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat, eh? A problem with this kind of analysis of risk as process, however, is that it misses risk processes where risk is a noun, not a verb:\n",
    "    \n",
    "    1. They took a risk\n",
    "    2. They ran a risk\n",
    "    3. It posed a risk\n",
    "    4. They put it at risk\n",
    " \n",
    "One of our search options, 'governor', can distinguish between these accurately. The query below shows us the function of risk words, and the lemma form of their governor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "govrole = interrogator(all_corpora, 'g', riskword, lemmatise = True, \n",
    "                       dep_type = 'collapsed_ccprocessed', quicksave = '6_govrole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print govrole['cht'].results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the result above:\n",
    "\n",
    "> `root:root` corresponds to risk as predicator.\n",
    "\n",
    "> `dobj:run/take/pose` are *run risk*, *take risk* and *pose risk*. Note that the risk word here is identified as a direct object, while in systemic functional grammar it would be a *process-range configuration*.\n",
    "\n",
    "> `prep_at:put` captures *to put at risk*. This only works when using the collapsed dependency grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can use this result for is to fix up our earlier count of risk by functional role. It's tricky, but shows the power of `editor()` and `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a copy, to be safe\n",
    "from copy import deepcopy\n",
    "syscopy = deepcopy(sysfunc)\n",
    "\n",
    "# for each corpus\n",
    "for k, v in syscopy.items():\n",
    "    # calculate number to add to process count\n",
    "    add_to_proc = govrole[k].results[['dobj:run', 'dobj:take', 'dobj:pose', 'prep_at:put']].sum()\n",
    "    # calculate number to subtract from participant count\n",
    "    subtract_from_part = govrole[k].results[['dobj:run', 'dobj:take', 'dobj:pose']].sum()\n",
    "    # calculate number to subtract from modifier count\n",
    "    subtract_from_mod = govrole[k].results['prep_at:put']\n",
    "    \n",
    "    # do these calculations\n",
    "    v.results['Process'] = v.results['Process'] + add_to_proc\n",
    "    v.results['Participant'] = v.results['Participant'] - subtract_from_part\n",
    "    v.results['Modifier'] = v.results['Modifier'] - subtract_from_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot more accurately, by role, and then by paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for role in ['Participant', 'Process', 'Modifier']:\n",
    "    df = entry_across_corpora(syscopy, role)\n",
    "    plotter('Frequency of risk as %s by newspaper' % role, df, style = 'fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, data in syscopy.items():\n",
    "    plotter('Functional roles of risk words in the %s' % name.upper(), data.results, style = 'fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk processes\n",
    "\n",
    "The next thing we can do with our `govrole` interrogation is to plot the frequencies of the five identified risk processes. To do this, we can rename the combination of role and governor to something more readable, and then remove all other entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "renames = {'root:root': 'to risk',\n",
    "           'dobj:run': 'to run risk',\n",
    "           'dobj:take': 'to take risk',\n",
    "           'dobj:pose': 'to pose risk',\n",
    "           'prep_at:put': 'to put at risk'}\n",
    "\n",
    "risk_processes = editor(govrole, replace_names = renames, \n",
    "                               just_entries = renames.values(), sort_by = 'total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying all this information properly is tricky. First, we can try collapsing distinctions between subcorpora---though this means that we can't observe longitudinal change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "risk_processes_c = editor(govrole, replace_names = renames, just_totals = True,\n",
    "                          just_entries = renames.values(), sort_by = 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotter('Risk processes', risk_processes_c, kind = 'pie', subplots = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can collapse the distinction between newspapers. Perhaps we could make another function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collapsed(interrogation):\n",
    "    import pandas as pd\n",
    "    return pd.concat([i for i in interrogation.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotter('Longitudinal behaviour of risk processes in U.S. print media', collapsed(risk_processes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also found growth in the use of risk as a nominal pre-head modifier (*risk factor*, *risk brokerage*, etc.). We can use the same interrogation to find out what risk modifies in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a bit of a hack: delete nnmod: from names, then remove \n",
    "# any entry with a ':' in it\n",
    "\n",
    "nom_mod = editor(govrole.results, replace_names = 'nnmod:', skip_entries = ':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotter('Nouns modified by risk as classifier', nom_mod.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to find out which words are increasing and decreasing the most over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nom_mod_inc = editor(govrole.results, replace_names = 'nnmod:', skip_entries = ':', sort_by = 'increase')\n",
    "nom_mod_dec = editor(govrole.results, replace_names = 'nnmod:', skip_entries = ':', sort_by = 'decrease')\n",
    "\n",
    "\n",
    "plotter('Nouns modified by risk as classifier, increasing', nom_mod_inc.results)\n",
    "plotter('Nouns modified by risk as classifier, decreasing', nom_mod_dec.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we noticed in our pilot investigation of the NYT was that while some adjectival risk words are declining in frequency (*risky* is a good example), others, like *at-risk* are becoming more prominent. We can check the other newspapers to see if the trend is general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "risky = interrogator(all_corpora, 'count', r'/(?i)\\brisk(y|ier|iest)/', quicksave = '6_risky')\n",
    "atrisk = interrogator(all_corpora, 'count', r'/(?i)\\bat-risk/', quicksave = '6_atrisk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mood role of risk words\n",
    "\n",
    "In our last investigation, we found that risk is increasingly occurring within complements and adjuncts, and less often within subject and finite/predicator positions. This was taken as evidence for decreasing arguability of risk in news discourse.\n",
    "\n",
    "We can attempt to replicate that result using a previous interrogation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we collapse the finite/predicator distinction because it's not very\n",
    "# well handled by dependency parses. this is a pity, since finite plays\n",
    "# a more important role in arguability than predicator.\n",
    "\n",
    "merges = {'Subject': roles.subject,\n",
    "          'Finite/Predicator': roles.finite + roles.predicator,\n",
    "          'Complement': roles.complement,\n",
    "          'Adjunct': roles.adjunct}\n",
    "\n",
    "moodrole = editor(riskfunct.results, '%', 'self', merge_entries = merges, just_entries = merges.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Risk and power\n",
    "\n",
    "The last thing we'll look at (for now) is the relationship beween risking and power.\n",
    "\n",
    "In our previous analysis, we found that powerful people are much more likely to do risking.\n",
    "\n",
    "To determine this, we needed to make two search queries. The first finds the nominal heads when risk/run risk/take risk is the process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = r'/^NN(S|)$/ !< /(?i).?\\brisk.?/ >># (@NP $ (VP <+(VP) (VP ( <<# (/VB.?/ < /(?i).?\\brisk.?/) ' \\\n",
    "    r'| <<# (/VB.?/ < /(?i)\\b(take|taking|takes|taken|took|run|running|runs|ran|put|putting|puts)/) < ' \\\n",
    "    r'(NP <<# (/NN.?/ < /(?i).?\\brisk.?/))))))'\n",
    "\n",
    "noun_riskers = interrogator(all_corpora, 'words', query, lemmatise = True, quicksave = '6_noun_riskers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, we need to get the frequencies of nouns generally, so that we can account for the fact that some nouns are very common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = r'/NN.?/ >># NP !< /(?i).?\\brisk.?/'\n",
    "noun_lemmata = interrogator(all_corpora, 'words', query, lemmatise = True, quicksave = '6_noun_lemmata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we can use `editor()` to combine the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rskrs = editor(noun_riskers.results, '%', noun_lemmata.results, just_totals = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we're using `editor()`'s `\"%\"` option, but instead of passing in a set of totals, we're passing in some results.\n",
    "\n",
    "When we do this, *corpkit* matches up every entry in dataframe1 with its equivalent in dataframe2, and works out the percentage individually. In this way, we account for the fact that *people* is a very common word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people = ['man', 'woman', 'child', 'baby', 'politician', \n",
    "          'senator', 'obama', 'clinton', 'bush']\n",
    "\n",
    "# change series name to name of publication\n",
    "for k, v in rskrs.items():\n",
    "    v.results.name = k\n",
    "\n",
    "# concatenate all our series\n",
    "combined = pd.concat([i.results for i in rskrs.values()])\n",
    "\n",
    "plotter('Risk and power', combined[people], num_to_plot = 'all', kind = 'bar', \n",
    "        x_label = 'Word', y_label = 'Risker percentage', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
